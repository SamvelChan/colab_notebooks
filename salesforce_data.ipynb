{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cd8c4cabeb7643109da7a87f38face91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Run Analysis!",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_cee0cc33c26a407dbdeb4e0d5ea18b5e",
            "style": "IPY_MODEL_b6694b74d5cb4c938af920eef303a400",
            "tooltip": ""
          }
        },
        "cee0cc33c26a407dbdeb4e0d5ea18b5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6694b74d5cb4c938af920eef303a400": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "026d915afb054d909c95299fd0ad1ed8": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_a1ed0f7c5c5a40ba8edd0c9738487dc3",
            "msg_id": "",
            "outputs": []
          }
        },
        "a1ed0f7c5c5a40ba8edd0c9738487dc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RuiCal/colab_notebooks/blob/main/salesforce_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SalesForce API Data Connector"
      ],
      "metadata": {
        "id": "U9HLuDaP_E8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classes"
      ],
      "metadata": {
        "id": "Jef4DAMW_Ad4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "%%capture\n",
        "# Install missing libraries in Colab\n",
        "!pip install simple-salesforce\n",
        "!pip install sidecar\n",
        "import os\n",
        "# This is a bad practice; we need to use a SalesForce API client Token instead;\n",
        "os.environ['SFDC_SID'] = '00D50000000JKHo!AQkAQP6HDGtfu2U_Uiwnab8UGFlAy8kBhT7JwDVbs5nE0UrTPGx7kB_EbxvC6nojIJ6cpGhcCOriv57ej0zsyNnll5XERjlm'\n",
        "os.environ['SLACK_BOT_TOKEN'] = ''"
      ],
      "metadata": {
        "id": "DShRdNw0_qQT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "922d3535-308b-4ebd-9ed9-170a3125a92c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 183 ms, sys: 31.3 ms, total: 214 ms\n",
            "Wall time: 23.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SalesForce Manager"
      ],
      "metadata": {
        "id": "N039jxgvbJOA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X11FYbdf-toG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc471677-a765-4339-e2cc-da6e941387b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "\n",
            "\u001b[32m2023-04-28 16:12:57.786760: Connected to https://udemy.my.salesforce.com/services/data/v52.0/\u001b[32m\n",
            "\n",
            "\u001b[32m2023-04-28 16:13:07.654160: Connected to https://udemy.my.salesforce.com/services/data/v52.0/\u001b[32m\n",
            "\n",
            "\u001b[32mQuerying SOQL Data\u001b[32m\n",
            "SOQL Query: \n",
            "\u001b[6;30;43mSELECT Id, Name FROM User WHERE email IN ('daniel.mcateer@udemy.com', 'garry.hickey@udemy.com', 'rui.calheiros@udemy.com', 'seth.klein@udemy.com', 'steven.reynolds@udemy.com', 'vamsi.pandari@udemy.com', 'ayse.ciftci@udemy.com', 'david.vaughan@udemy.com', 'ryan.petigura@udemy.com', 'joao.carlos@udemy.com', 'erin.ohern@udemy.com', 'paul.sleeman@udemy.com', 'sawandi.cassell@udemy.com', 'jenica.draney@udemy.com', 'christopher.mcdonald@udemy.com', 'dan.fortner@udemy.com', 'kei.pan@udemy.com', 'evelyn.pan@udemy.com', 'william.holton@udemy.com', 'james.caudery@udemy.com', 'kuldeep.harsh@udemy.com', 'lynsie.zellmer@udemy.com', 'sarah.sparks@udemy.com')\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "''' Import Libraries '''\n",
        "# -*- Data Manipulation  -*-\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import dask.dataframe as dd\n",
        "\n",
        "#  -*- I/O, Error Handling -*-\n",
        "import os, sys, json, traceback\n",
        "from os import path\n",
        "import warnings, time\n",
        "from pathlib import Path \n",
        "from datetime import datetime, date \n",
        "from collections import OrderedDict\n",
        "from google.colab import drive\n",
        "\n",
        "# -* To open separate Tabs in Jupyter Notebook -*-\n",
        "from sidecar import Sidecar\n",
        "\n",
        "# -*- Set visual styler options for Pandas DataFrame outputs -*- \n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "cm = sns.color_palette('viridis', as_cmap=True)\n",
        "\n",
        "# -*- Simple-SalesForce API -*-\n",
        "from simple_salesforce import Salesforce \n",
        "from simple_salesforce.exceptions import SalesforceExpiredSession\n",
        "\n",
        "# -*- Set Notebook Options -*- \n",
        "pd.set_option('display.max_columns', None)  \n",
        "pd.set_option('display.max_info_columns', 500)  \n",
        "pd.set_option('display.max_rows', None) \n",
        "from IPython.display import HTML, display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# -*- Fast selecting and setting with Pandas DataFrame Objects using loc or iloc -*-  \n",
        "idx = pd.IndexSlice \n",
        "All = slice(None)\n",
        "\n",
        "# -*- Constants\n",
        "GOOGLE_PATH = '/content/gdrive/'\n",
        "SESSION_URL = 'https://udemy.my.salesforce.com/secur/frontdoor.jsp?'\n",
        "\n",
        "\n",
        "class sf_Manager():\n",
        "    \n",
        "    def __init__(self, **kwargs):\n",
        "        \n",
        "        '''\n",
        "            Description:\n",
        "                Initialize SalesForce Manager Class with kwargs parameters, plus session parameters for SalesForce Authentication Session\n",
        "\n",
        "                Kwargs Required parameters:\n",
        "                    * [acct_labels, opp_labels]: SalesForce SOQL Account and Opportunity fields to query\n",
        "\n",
        "                SalesForce Authentication:\n",
        "                    *username: the Salesforce username to use for authentication;\n",
        "                    *password: the Salesforce password for the username;\n",
        "                    *token: the Salesforce security API token for the username (retrieved from Salesforce UI) # does not work; disabled in our SalesForce instance;\n",
        "                    *instance: Domain of your Salesforce instance, e.g.`domain1.salesforce.com`;\n",
        "                    *sid: Session ID for Auth; retrieve SID from set-cookie headers in https://udemy.my.salesforce.com/secur/frontdoor.jsp?\n",
        "\n",
        "                Data Manipulation:\n",
        "                    *inr_to_usd: to convert Opportunities in Indian Currenty to USD.\n",
        "                    *account_fields: not used with any method, but handy to print all fields for the SalesForce Account object\n",
        "                    *opportunity_fields: not used with any method, but handy to print all fields for the SalesForce Opportunity object\n",
        "                    *case_fields: not used with any method, but handy to print all fields for the SalesForce Case object\n",
        "                    *SSEIDs: SalesForce User account IDs for the S&SE Team; used with Cases DataFrame object\n",
        "                    *years: default Years to retrieve Opportunity Data from SOQL Query\n",
        "        '''\n",
        "        \n",
        "        super(sf_Manager, self).__init__()\n",
        "        self.kwargs = kwargs\n",
        "        \n",
        "        # Mount Google Drive\n",
        "        self._mount_google_drive()\n",
        "\n",
        "        # Set Google Colab to Wrap Text on Outputs\n",
        "        def set_css():\n",
        "          display(HTML('''\n",
        "          <style>\n",
        "            pre {\n",
        "                white-space: pre-wrap;\n",
        "            }\n",
        "          </style>\n",
        "          '''))\n",
        "        get_ipython().events.register('pre_run_cell', set_css)\n",
        "\n",
        "        # SalesForce User Creds: these are in set in your SalesForce User Profile settings\n",
        "        self.instance = 'udemy.my.salesforce.com'\n",
        "        self.sid = os.environ['SFDC_SID']\n",
        "\n",
        "        # Get Session\n",
        "        self.sf = self.conn()\n",
        "        \n",
        "        # Get S&SE Assignments Map\n",
        "        self.sse_salesforce_opp_assignments_path = GOOGLE_PATH + 'Shareddrives/UB/Sales/Sales & Solution Engineers/Sales/SalesForce/sse_salesforce_opp_assignments_2023v1.json'\n",
        "        self.sse_assignments = self._open_json_file(path_or_buf=self.sse_salesforce_opp_assignments_path)\n",
        "        \n",
        "        # Used for Currency conversion for India Opp ARR\n",
        "        self.inr_to_usd = .014\n",
        "         \n",
        "        # These are the available Salesforce Fields for each Entity Type (e.g., Account, Opportunity, Case)\n",
        "        # More info here: https://developer.salesforce.com/docs/atlas.en-us.234.0.sfFieldRef.meta/sfFieldRef/salesforce_field_reference.htm\n",
        "        self.user_fields = \",\".join([field['name'] for field in self.sf.User.describe()['fields']])\n",
        "        self.account_fields = \",\".join([field['name'] for field in self.sf.Account.describe()['fields']])\n",
        "        self.opportunity_fields = \",\".join([field['name'] for field in self.sf.Opportunity.describe()['fields']])\n",
        "        self.case_fields = \",\".join([field['name'] for field in self.sf.Case.describe()['fields']])\n",
        "        self.contact_fields = \",\".join([field['name'] for field in self.sf.Contact.describe()['fields']])\n",
        "        self.ironclad_workflow_fields = \",\".join([field['name'] for field in self.sf.ironclad__Ironclad_Workflow__c.describe()['fields']])\n",
        "        self.ironclad__Ironclad_Approval_fields = \",\".join([field['name'] for field in self.sf.ironclad__Ironclad_Approval__c.describe()['fields']])\n",
        "        self.OpportunityChangeEvent_fields = \",\".join([field['name'] for field in self.sf.OpportunityChangeEvent.describe()['fields']])\n",
        "        self.OpportunityFieldHistory_fields = \",\".join([field['name'] for field in self.sf.OpportunityFieldHistory.describe()['fields']])\n",
        "        self.OpportunityStage_fields = \",\".join([field['name'] for field in self.sf.OpportunityStage.describe()['fields']])\n",
        "        self.OpportunityHistory_fields = \",\".join([field['name'] for field in self.sf.OpportunityHistory.describe()['fields']])\n",
        "        self.OpportunityLineItem_fields = \",\".join([field['name'] for field in self.sf.OpportunityLineItem.describe()['fields']])\n",
        "        self.Product2_fields =  \",\".join([field['name'] for field in self.sf.Product2.describe()['fields']])\n",
        "        self.Task_fields =  \",\".join([field['name'] for field in self.sf.Task.describe()['fields']])\n",
        "        \n",
        "        # Get SalesForce User IDs for the S&SE Team; \n",
        "        # Use these IDs to filter SalesForce SOQL Query for S&SE assigned Cases\n",
        "        soql_query = \"SELECT Id, Name FROM User WHERE email IN \" + \\\n",
        "                     \"('daniel.mcateer@udemy.com', 'garry.hickey@udemy.com', 'rui.calheiros@udemy.com', 'seth.klein@udemy.com', 'steven.reynolds@udemy.com', \" + \\\n",
        "                     \"'vamsi.pandari@udemy.com', 'ayse.ciftci@udemy.com', 'david.vaughan@udemy.com', 'ryan.petigura@udemy.com', 'joao.carlos@udemy.com', \" + \\\n",
        "                     \"'erin.ohern@udemy.com', 'paul.sleeman@udemy.com', 'sawandi.cassell@udemy.com', 'jenica.draney@udemy.com', 'christopher.mcdonald@udemy.com', \" \\\n",
        "                     \"'dan.fortner@udemy.com', 'kei.pan@udemy.com', 'evelyn.pan@udemy.com', 'william.holton@udemy.com', 'james.caudery@udemy.com', 'kuldeep.harsh@udemy.com', \" \\\n",
        "                     \"'lynsie.zellmer@udemy.com', 'sarah.sparks@udemy.com')\" \n",
        "        \n",
        "        self.SSE_IDs = self.dict_to_df(soql_result=self.query(query=soql_query))\n",
        "        self.SSE_IDs.loc[len(self.SSE_IDs.index)] = ['00G38000002vdbeEAA', 'UFB SE Request',]\n",
        "        self.SSE_IDs.loc[len(self.SSE_IDs.index)] = ['00G38000002bt1KEAQ', 'UFB Request for Proposal']\n",
        "        # sys.stdout.write('--------------------\\n{0}\\n--------------------'.format(self.SSE_IDs))\n",
        "        self.SSEIDs = list(self.SSE_IDs.Id)\n",
        "        self.SSEIDs = \", \".join([\"'{}'\".format(element) for element in self.SSEIDs])\n",
        "        \n",
        "        # Define SOQL Syntax fields to query Accounts\n",
        "        acct_labels = ['o.Account.{}'.format(field) for field in self.kwargs.get('acct_labels').split(',')]\n",
        "        acct_labels = \",\".join([field for field in acct_labels]) \n",
        "        acct_labels =  \"\".join(acct_labels.split())\n",
        "        acct_labels = acct_labels.replace(\",\", \", \")          \n",
        "        self.acct_labels = acct_labels\n",
        "        \n",
        "        # Define SOQL Syntax fields to query Opportunities\n",
        "        opp_labels = ['o.{}'.format(field) for field in self.kwargs.get('opp_labels').split(',')]\n",
        "        opp_labels = \",\".join([field for field in opp_labels]) \n",
        "        opp_labels = \"\".join(opp_labels.split())\n",
        "        opp_labels = opp_labels.replace(\",\", \", \")\n",
        "        self.opp_labels = opp_labels\n",
        "        \n",
        "        # Default years to filter SalesForce Opportunities\n",
        "        self.years = [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026]\n",
        "        \n",
        "    def conn(self):\n",
        "        '''\n",
        "        Connects to SalesForce Web Services using simple_salesforce library and returns session \n",
        "        \n",
        "        Parameters:\n",
        "            *username (string): Salesforce username to use for authentication\n",
        "            *password (sting):  Salesforce password for the username\n",
        "            *token(string): Salesforce security API token for the username (retrieved from Salesforce UI)\n",
        "            *instance (string): Domain of your Salesforce instance, e.g.`domain1.salesforce.com`\n",
        "        \n",
        "        Returns: \n",
        "            SalesForce API Session\n",
        "        '''\n",
        "        try:\n",
        "            '''\n",
        "            This auth method now fails because SF Admin team disabled user-level authentication\n",
        "            self.sf = Salesforce(username=self.username,\n",
        "                                 password=self.password, \n",
        "                                 security_token=self.token, \n",
        "                                 instance=self.instance\n",
        "                                )\n",
        "            '''\n",
        "            session = Salesforce(instance=self.instance, session_id=self.sid, version='52.0')\n",
        "            sys.stdout.write('\\n{0}{1}: Connected to {2}{3}\\n'.format('\\x1b[32m', datetime.now(), session.base_url, '\\x1b[32m'))\n",
        "            return session\n",
        "    \n",
        "        except SalesforceExpiredSession as error:\n",
        "            self.sf = Salesforce(instance=self.instance, session_id=self.sid)\n",
        "            return self.sf\n",
        "            sys.stdout.write('\\n {0} *** Error in <sf_Manager.conn()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc())) \n",
        "    \n",
        "    def query(self, query):\n",
        "        \n",
        "        '''\n",
        "        Returns SalesForce OrderedDict Object Query\n",
        "        \n",
        "        Parameters:\n",
        "            query (string): SalesForce Query (SOQL)\n",
        "        \n",
        "        Returns:\n",
        "            result (OrderedDict): SalesForce OrderedDict Object\n",
        "        '''\n",
        "        \n",
        "        try:\n",
        "            self.conn()\n",
        "            sys.stdout.write('\\n{0}Querying SOQL Data{1}'.format('\\x1b[32m', '\\x1b[32m'))\n",
        "            sys.stdout.write('\\nSOQL Query: \\n{0}{1}{2}\\n'.format('\\x1b[6;30;43m', query, '\\x1b[0m'))\n",
        "            result = self.sf.query_all(query)\n",
        "            return result\n",
        "        except SalesforceExpiredSession as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <sf_Manager.query()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))   \n",
        "            return dict()\n",
        "    \n",
        "    def dict_to_df(self, soql_result):\n",
        "        \n",
        "        '''\n",
        "        Returns Pandas DataFrame object of a SalesForce OrderedDict query\n",
        "        \n",
        "        Parameters:\n",
        "            soql_result: (OrderedDict): SalesForce API OrderedDict Object\n",
        "        \n",
        "        Returns:\n",
        "            df (Pandas DataFrame): Pandas DataFrame object of soql_result\n",
        "        '''       \n",
        "        try:\n",
        "            items = {val: dict(soql_result['records'][val]) for val in range(soql_result['totalSize'])}\n",
        "            df = pd.DataFrame.from_dict(items, orient='index').drop(['attributes'], axis=1)\n",
        "            return df\n",
        "        \n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <sf_Manager.dict_to_df()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))            \n",
        "    \n",
        "    def get_salesforce_object_entities(self):\n",
        "        \n",
        "        '''\n",
        "        Description: Function that returns metadata for all SalesForce API Objects\n",
        "        Parameters:     \n",
        "             None\n",
        "        Returns:\n",
        "            sf_objects (OrderedDict) = SalesForce Data Model Objects (e.g., Account, Opportunity, Case)\n",
        "        '''\n",
        "        try:\n",
        "            sf = self.conn()\n",
        "            sf_objects = []\n",
        "            for sfobject in sf.describe()['sobjects']:\n",
        "                sf_objects.append(sfobject['name']) \n",
        "            return sf_objects\n",
        "        \n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <sf_Manager.get_salesforce_object_entities()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc())) \n",
        "\n",
        "    def get_users(self):\n",
        "        \n",
        "        '''\n",
        "        Queries SalesForce User data and Returns Pandas DataFrame as result\n",
        "           \n",
        "           Parameters:\n",
        "                None\n",
        "            \n",
        "            Returns:\n",
        "                users (Pandas DataFrame): Pandas DataFrame Object of SalesForce Users data\n",
        "        '''\n",
        "        try:\n",
        "            sys.stdout.write('\\n********  {0}Getting SalesForce Users{1}  ********\\n'.format('\\x1b[6;30;43m', '\\x1b[0m'))\n",
        "            soql_query = \"SELECT Id, Name, Title, Department, Manager_for_Reports__c, Email FROM User\"\n",
        "            soql_result = self.query(query=soql_query)\n",
        "            users = self.dict_to_df(soql_result=soql_result)\n",
        "            users.columns = 'user_' + users.columns\n",
        "            print(users.info())\n",
        "            return users\n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <sf_Manager.get_users()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc())) \n",
        "            return pd.DataFrame()\n",
        "        \n",
        "    def get_accounts(self):\n",
        "        \n",
        "        '''\n",
        "        Queries SalesForce Accounts Data and Returns Pandas DataFrame as result\n",
        "        \n",
        "        Parameters: \n",
        "                None\n",
        "        \n",
        "        Returns: \n",
        "                accounts (Pandas DataFrame): Pandas DataFrame Object of SalesForce Accounts OrderedDict Account data\n",
        "        '''        \n",
        "        try:\n",
        "            data = pd.DataFrame()\n",
        "            sys.stdout.write('\\n********  {0}Getting Accounts{1}  ********\\n'.format('\\x1b[6;30;43m', '\\x1b[0m'))\n",
        "            soql_query = \"SELECT \" + self.kwargs.get('account_labels') + \" FROM Account\"\n",
        "            data = self.dict_to_df(soql_result=self.query(query=soql_query))\n",
        "            data.columns = 'acct_' + data.columns\n",
        "            data.sort_index(axis=1, ascending=True, inplace=True)\n",
        "            print(data.info())\n",
        "            print('\\nNo. of Accounts: {}'.format(len(data)))\n",
        "            return data\n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <sf_Manager.get_accounts()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc())) \n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def get_accounts_opportunities(self, years=[pd.Timestamp(datetime.now()).year]):\n",
        "        \n",
        "        '''\n",
        "        Queries SalesForce Accounts & Opportunities in single call and converts results to Pandas DataFrame objects\n",
        "        \n",
        "        Parameters:\n",
        "            years (list(int)): Years to filter SalesForce Account and Opportunity Query\n",
        "       \n",
        "        Returns:\n",
        "            Accounts (Pandas DataFrame): Pandas DataFrame object of SalesForce Accounts data (parent records)\n",
        "            Opportunities (Pandas DataFrame): Pandas DataFrame object with Opportunity Data (child records)\n",
        "        ''' \n",
        "        \n",
        "        try:\n",
        "            \n",
        "            # (1) Get SalesForce Opportunities data; (2) Convert OrderedDict result to Pandas DataFrame\n",
        "            opportunities = pd.DataFrame()\n",
        "            sys.stdout.write('\\n\\n******** {0}Getting Accounts & Opps Data for {1}{2} ********\\n\\n'.format('\\x1b[6;30;43m', years, '\\x1b[0m'))\n",
        "            for year in years:\n",
        "                soql_query = \"SELECT \" + str(self.opp_labels) + \", \" + str(self.acct_labels) + \" FROM Opportunity o WHERE o.FiscalYear IN (\" + str(year) + \")\"\n",
        "                soql_results = self.dict_to_df(soql_result=sf.query(query=soql_query))\n",
        "                sys.stdout.write('\\n{0}{1} No. of Opportunities: {2} {3}\\n'.format('\\x1b[6;30;46m',  year, len(soql_results), '\\x1b[0m'))\n",
        "                opportunities = pd.concat([opportunities, soql_results], ignore_index=True, sort=False)\n",
        "       \n",
        "            # Clean up data\n",
        "            opportunities.columns = 'opp_' + opportunities.columns # add prefix to make it easier to distinguish data features from each object\n",
        "            opportunities.drop_duplicates(subset=['opp_Id'], inplace=True)  # drop any duplicate Opps\n",
        "            opportunities.dropna(subset=['opp_AccountId'],inplace=True) # Some Opportunities aren't tagged with an Account, drop these records\n",
        "            \n",
        "            # Extract the nested Accounts Data from query results\n",
        "            sys.stdout.write('\\n{}>> Parsing Accounts and Opportunities Records. . . {}\\n'.format('\\x1b[6;30;46m', '\\x1b[0m'))\n",
        "            accounts = {}\n",
        "            accounts['records'] = opportunities['opp_Account'].to_list()\n",
        "            accounts['totalSize'] = len(opportunities)\n",
        "            accounts['done'] = True\n",
        "            \n",
        "            # Convert Accounts Dict to Pandas DataFrame\n",
        "            accounts = self.dict_to_df(soql_result=accounts)    \n",
        "            accounts.columns = 'acct_' + accounts.columns\n",
        "            accounts.drop_duplicates(subset=['acct_Id'], inplace=True)\n",
        "            \n",
        "            # Print Results\n",
        "            sys.stdout.write('\\n\\n -*- {0} Account Records {1} -*-\\n\\n'.format('\\x1b[6;30;46m','\\x1b[0m'))\n",
        "            print(accounts.info())\n",
        "            sys.stdout.write('\\n\\n -*- {0} Opportunity Records {1} -*-\\n\\n'.format('\\x1b[6;30;46m', '\\x1b[0m'))\n",
        "            print(opportunities.info())\n",
        "            return accounts, opportunities\n",
        "        \n",
        "        except Exception as error:   \n",
        "            sys.stdout.write('\\n {0} *** Error: <sf_Manager.get_accounts_opportunities()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc())) \n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def get_opportunity_field_history(self, fields=['StageName']):\n",
        "        '''\n",
        "        Queries SalesForce OpportunityFieldHistory Object and returns results as Pandas DataFrame object\n",
        "        \n",
        "        Parameters:\n",
        "            fields (list): List of Fields to return changes for in the Opportunity\n",
        "       \n",
        "        Returns:\n",
        "            Data (Pandas DataFrame): OpportunityFieldHistory\n",
        "        ''' \n",
        "        try:\n",
        "            data = pd.DataFrame()\n",
        "            sys.stdout.write('\\n\\n******** {0} Getting Opportunity Field History Data {1}  ******** \\n'.format('\\x1b[6;30;43m', '\\x1b[0m'))\n",
        "            fields = \", \".join([\"'{}'\".format(element) for element in fields])\n",
        "            soql_query = \"SELECT \" + self.OpportunityFieldHistory_fields + \" FROM OpportunityFieldHistory WHERE Field IN (\" + fields + \") AND OpportunityId='0061T000015aml1QAA'\"\n",
        "            data = sf.dict_to_df(soql_result=sf.query(query=soql_query))\n",
        "            data.columns = 'oppfieldhist_' + data.columns\n",
        "            \n",
        "            # Convert to DateTime\n",
        "            date_labels = ['oppfieldhist_CreatedDate']\n",
        "            data[date_labels] = data[date_labels].apply(pd.to_datetime)\n",
        "            data['oppfieldhist_CreatedDate_'] = data['oppfieldhist_CreatedDate']\n",
        "            data.sort_values(by=['oppfieldhist_CreatedDate'], ascending=[True], inplace=True)\n",
        "            data.set_index(keys=['oppfieldhist_CreatedDate'], inplace=True)\n",
        "            data.sort_index(axis=1, ascending=True, inplace=True)\n",
        "            sys.stdout.write('\\n\\n -*- {0}Opportunity Field History Records{1} -*- \\n\\n'.format('\\x1b[6;30;46m','\\x1b[0m'))\n",
        "            print(data.info())\n",
        "            return data\n",
        "        \n",
        "        except Exception as error:   \n",
        "            sys.stdout.write('\\n {0} *** Error: <sf_Manager.get_opportunity_field_history()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc())) \n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def get_opportunity_history(self):\n",
        "        '''\n",
        "        Queries SalesForce Opportunity History Object and returns results as Dask DataFrame object\n",
        "        \n",
        "        Parameters:\n",
        "            None\n",
        "       \n",
        "        Returns:\n",
        "            Data (Dask DataFrame): Opportunity History Records (Large DataSet)\n",
        "        ''' \n",
        "        try:\n",
        "            data = pd.DataFrame()\n",
        "            sys.stdout.write('\\n\\n******** {0} Getting Opportunity History Data {1}  ******** \\n'.format('\\x1b[6;30;43m', '\\x1b[0m'))\n",
        "            \n",
        "            # For later, update the SOQL to return only records which are updated since last call; i.e., add WHERE condition to the below SOQL for updatedDate of records. Ortherwise, it will pull all 1.1M rows each time\n",
        "            soql_query = \"SELECT \" + self.OpportunityHistory_fields + \" FROM OpportunityHistory\"\n",
        "            data = sf.dict_to_df(soql_result=sf.query(query=soql_query))\n",
        "            data.columns = 'opphist_' + data.columns\n",
        "            \n",
        "            # Convert to DateTime: VERY IMPORTANT for fast processing\n",
        "            date_labels = ['opphist_CreatedDate']\n",
        "            data[date_labels] = data[date_labels].apply(pd.to_datetime)\n",
        "            data['opphist_CreatedDate_'] = data['opphist_CreatedDate']\n",
        "            data.sort_values(by=['opphist_CreatedDate'], ascending=[True], inplace=True)\n",
        "            data.set_index(keys=['opphist_CreatedDate'], inplace=True)\n",
        "            \n",
        "            data.sort_index(axis=1, ascending=True, inplace=True)\n",
        "            sys.stdout.write('\\n\\n -*- {0}Opportunity History Records{1} -*- \\n\\n'.format('\\x1b[6;30;46m','\\x1b[0m'))\n",
        "            print(data.info())\n",
        "            \n",
        "            # Export Data to Google Drive, as parquet GZip file\n",
        "            fpath = GOOGLE_PATH + 'Shareddrives/UB/Sales/Sales & Solution Engineers/Sales/Analysis/data/opps_cases/opp_history.parquet.gzip'\n",
        "            data.to_parquet(path=fpath, index=True)\n",
        "            sys.stdout.write('\\n\\n -*- {0}Exporting Parquet File: {2} . . . {1} -*- \\n\\n'.format('\\x1b[6;30;46m','\\x1b[0m', fpath))\n",
        "            \n",
        "            # Returns Dask Dataframe\n",
        "            return dd.from_pandas(data, npartitions=5)\n",
        "        \n",
        "        except Exception as error:   \n",
        "            sys.stdout.write('\\n {0} *** Error: <sf_Manager.get_opportunity_history()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc())) \n",
        "            return pd.DataFrame() \n",
        "        \n",
        "    def get_opportunity_products(self):\n",
        "        '''\n",
        "        Returns SalesForce Opportunity Line Items (i.e., Products)\n",
        "        \n",
        "        Parameters:\n",
        "            None\n",
        "       \n",
        "        Returns:\n",
        "            Data (Pandas DataFrame): Opportunity Line Items (i.e., Products)\n",
        "        ''' \n",
        "        try:\n",
        "            data = pd.DataFrame()\n",
        "            sys.stdout.write('\\n\\n******** {0} Getting Opportunity Line Items (i.e., Products) Data {1}  ******** \\n'.format('\\x1b[6;30;43m', '\\x1b[0m'))\n",
        "            soql_query = soql = \"SELECT Id, CreatedDate, OpportunityId, Product2Id, ProductCode, Quantity, TotalPrice, Annual_per_User_License_Cost__c, Live_Annual_Recurring_Revenue__c, Term_Start_Date__c, Term_End_Date__c FROM OpportunityLineItem\"\n",
        "            data = sf.dict_to_df(soql_result=sf.query(query=soql_query))\n",
        "            data.columns = 'oppli_' + data.columns\n",
        "        \n",
        "            sys.stdout.write('\\n\\n -*- {0}Opportunity Line Items (i.e., Products) Records{1} -*- \\n\\n'.format('\\x1b[6;30;46m','\\x1b[0m'))       \n",
        "            print(data.info())\n",
        " \n",
        "            return data\n",
        "        \n",
        "        except Exception as error:   \n",
        "            sys.stdout.write('\\n {0} *** Error: <sf_Manager.get_opportunity_products()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc())) \n",
        "            return pd.DataFrame()  \n",
        "        \n",
        "    def get_product_details(self):\n",
        "        '''\n",
        "        Returns SalesForce Product2 Records (i.e., ['Subscription Services', None, 'UPro', 'Language Pack', 'Services', 'Core', 'EY Skills Foundry', 'Non-Subscription Services', 'Content'])\n",
        "        \n",
        "        Parameters:\n",
        "            None\n",
        "       \n",
        "        Returns:\n",
        "            Data (Pandas DataFrame): SalesForce Product2 Records (i.e., UB Product types such as UPro)\n",
        "        ''' \n",
        "        try:\n",
        "            data = pd.DataFrame()\n",
        "            sys.stdout.write('\\n\\n******** {0} Getting Products Data {1}  ******** \\n'.format('\\x1b[6;30;43m', '\\x1b[0m'))\n",
        "            soql_query = soql = \"SELECT Id, Name, Family FROM Product2\"\n",
        "            data = sf.dict_to_df(soql_result=sf.query(query=soql_query))\n",
        "            data.columns = 'prod_' + data.columns\n",
        "        \n",
        "            sys.stdout.write('\\n\\n -*- {0}Opportunity Products Records{1} -*- \\n\\n'.format('\\x1b[6;30;46m','\\x1b[0m'))       \n",
        "            print(data.info())\n",
        " \n",
        "            return data\n",
        "        \n",
        "        except Exception as error:   \n",
        "            sys.stdout.write('\\n {0} *** Error: <sf_Manager.get_opportunity_products()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc())) \n",
        "            return pd.DataFrame()\n",
        "        \n",
        "    def get_cases(self, soql_query=\"SELECT Id FROM Case LIMIT 1\"):\n",
        "        \n",
        "        '''\n",
        "        Queries SalesForce API for Case data\n",
        "        \n",
        "        Parameters:\n",
        "            soql_query (string): SalesForce Object Query Language query\n",
        "        \n",
        "        Returns: \n",
        "            data (Pandas DataFrame): SalesForce Case Records \n",
        "        '''        \n",
        "        \n",
        "        try:\n",
        "            data = pd.DataFrame()\n",
        "            sys.stdout.write('\\n******** {0}Getting Case Data{1}  ******** \\n'.format('\\x1b[6;30;43m', '\\x1b[0m'))\n",
        "            data = sf.dict_to_df(soql_result=sf.query(query=soql_query))\n",
        "            data.columns = 'case_' + data.columns\n",
        "            data.sort_index(axis=1, ascending=True, inplace=True)\n",
        "            \n",
        "            # Print results\n",
        "            sys.stdout.write('\\n\\n -*- {0}Case Records{1} -*- \\n\\n'.format('\\x1b[6;30;46m','\\x1b[0m'))\n",
        "            print(data.info())\n",
        "            return data\n",
        "        \n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <sf_Manager.get_cases()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "    def get_ironclad_workflows(self):\n",
        "        \n",
        "        '''\n",
        "        Queries SalesForce IronClad Workflows and related Cases\n",
        "        \n",
        "        Parameters:\n",
        "            None \n",
        "        \n",
        "        Results: \n",
        "            data (Pandas DataFrame): SalesForce IronClad Workflow Cases\n",
        "        '''        \n",
        "        try:\n",
        "    \n",
        "            # Get IronClad Workflows\n",
        "            sys.stdout.write('\\n ********  {0} Getting iron_clad_workflows Data {1}  ******** \\n'.format('\\x1b[6;30;43m', '\\x1b[0m'))\n",
        "            soql_query = \"SELECT iw.id, iw.Name, iw.CreatedDate, iw.ironclad__Workflow_ID__c, iw.ironclad__Workflow_Name__c, iw.ironclad__Workflow_Step__c, iw.ironclad__Workflow_Link__c, iw.Case__c FROM ironclad__Ironclad_Workflow__c iw WHERE iw.Case__c != null\"\n",
        "            IW = self.dict_to_df(soql_result=self.query(query=soql_query))\n",
        "            IW.columns = 'ironclad_workflow_' + IW.columns\n",
        "            print(IW.info())\n",
        "            \n",
        "            # Get IronClad Workflow Approvals\n",
        "            sys.stdout.write('\\n ********  {0} Getting ironclad__Ironclad_Approval__c {1}  ******** \\n'.format('\\x1b[6;30;43m', '\\x1b[0m'))\n",
        "            soql = \"SELECT ia.Id, ia.IsDeleted, ia.Name, ia.CurrencyIsoCode, ia.CreatedDate, ia.CreatedById, ia.LastModifiedDate, ia.LastModifiedById, ia.SystemModstamp, ia.LastActivityDate, ia.ironclad__Ironclad_Workflow__c, ia.ironclad__Approval_Name__c, ia.ironclad__Approval_Role__c, ia.ironclad__Approval_Status__c FROM ironclad__Ironclad_Approval__c ia\"\n",
        "            IA = sf.dict_to_df(soql_result=sf.query(query=soql))\n",
        "            IA.columns = 'ironclad_workflow_approval_' + IA.columns\n",
        "            IA.sort_index(axis=1, ascending=True, inplace=True)\n",
        "            print(IA.info())\n",
        "            \n",
        "            # Merge Approvals with Workflows\n",
        "            sys.stdout.write('\\n ********  {0} Merging iron_clad_workflows & ironclad__Ironclad_Approval__c {1}  ******** \\n'.format('\\x1b[6;30;43m', '\\x1b[0m'))\n",
        "            aggs_ = pd.merge(IW, IA, how='left', left_on='ironclad_workflow_Id', right_on='ironclad_workflow_approval_ironclad__Ironclad_Workflow__c')\n",
        "            aggs_.drop_duplicates(subset=['ironclad_workflow_Id', 'ironclad_workflow_approval_Id'], inplace=True)\n",
        "            aggs_.sort_index(axis=1, ascending=True, inplace=True)\n",
        "            print(aggs_.info())\n",
        "            del IW, IA\n",
        "            return aggs_\n",
        "        \n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <sf_Manager.get_ironclad_workflows()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "    def get_clean_merged_data(self, accounts=pd.DataFrame(), opportunities=pd.DataFrame(), cases=pd.DataFrame()):\n",
        "        \n",
        "        '''\n",
        "        Cleans and Merges SalesForce Accounts, Opportunities, and Cases records. \n",
        "\n",
        "            Parameters:\n",
        "                accounts (Pandas DataFrame object): SalesForce Account Records retrieved from SFDC API (SOQL) as a Pandas DataFrame\n",
        "                opportunities (Pandas DataFrame object): SalesForce Opportunity Records retrieved from SFDC API (SOQL) as a Pandas DataFrame\n",
        "                cases (Pandas DataFrame object): SalesForce Case Records retrieved from SFDC API (SOQL) as a Pandas DataFrame\n",
        "             \n",
        "            Returns:\n",
        "               1. account_opps_cases (Pandas DataFrame): aggs_ Account, Opportunity, Case records, and matching Legal IronClad Workflows, and type of Udemy Product tied to each Opportunity \n",
        "        '''       \n",
        "\n",
        "        try:\n",
        "            # Main DataFrame for all aggs_ data\n",
        "            account_opps = pd.DataFrame()\n",
        "            \n",
        "            # Additional SalesForce Data Merge with Account, Opportunity, and Case records\n",
        "            users = self.get_users()\n",
        "            iron_clad_workflows = self.get_ironclad_workflows()\n",
        "            opportunity_products = self.get_opportunity_products()\n",
        "            product_details = self.get_product_details()\n",
        "\n",
        "            # -*- -------------------ACCOUNT DATA ------------------- -*-\n",
        "\n",
        "            sys.stdout.write('\\n\\n{0} Cleaning Accounts Data {1}\\n'.format('\\x1b[6;30;43m', '\\x1b[0m'))\n",
        "            # To retrieve Column index by location; not used elsewhere in this Class at the moment;\n",
        "            idx_dic = {}\n",
        "            for col in accounts.columns:\n",
        "                idx_dic[col] = accounts.columns.get_loc(col)\n",
        "                \n",
        "            # Clean fields with NaN\n",
        "            acct_values = {\n",
        "                            'acct_NumberOfEmployees': 0, 'acct_AnnualRevenue': 0,'acct_Employee_Segment__c': 'nan',\n",
        "                            'acct_Region_New__c': 'nan', 'acct_Region__c': 'nan', 'acct_Business_Segment__c': 'nan'\n",
        "                          }\n",
        "            accounts.fillna(value=acct_values, inplace=True)\n",
        "            \n",
        "            # Convert to int32\n",
        "            accounts = accounts.astype({'acct_NumberOfEmployees': 'int32'}).copy()\n",
        "            \n",
        "            # Clean dirty data df.loc[df.column_a == 44, 'column_b'] = 100\n",
        "            is_acct_Region_New__c_India = (accounts.acct_Region_New__c == 'India')\n",
        "            accounts.loc[idx[is_acct_Region_New__c_India], 'acct_Region_New__c'] = 'APAC'\n",
        "            is_acct_Region_New__c_US_West = (accounts.acct_Region_New__c == 'USâ€“West')\n",
        "            accounts.loc[idx[is_acct_Region_New__c_US_West], 'acct_Region_New__c'] = 'AMER'\n",
        "            is_acct_Business_Segment__c_Commericial = (accounts.acct_Business_Segment__c == 'Commericial')\n",
        "            accounts.loc[idx[is_acct_Business_Segment__c_Commericial], 'acct_Business_Segment__c'] = 'Commercial'\n",
        "           \n",
        "            # Convert to Categorical Data Type: faster performance\n",
        "            categorical_labels = ['acct_Region_New__c', 'acct_Employee_Segment__c', 'acct_Business_Segment__c']\n",
        "            accounts[categorical_labels] = accounts[categorical_labels].apply(pd.Categorical)\n",
        "            \n",
        "            # -*- -------------------- OPPORTUNITY DATA ------------------- -*-\n",
        "\n",
        "            # Clean fields with NaN\n",
        "            sys.stdout.write('\\n\\n{0} Cleaning Opportunities Data {1}\\n'.format('\\x1b[6;30;43m', '\\x1b[0m'))\n",
        "            missing_values = {\n",
        "                            'opp_SBCF_Prior_License_Qty__c': 0, 'opp_Type': 'none', 'opp_FiscalYear': 0,\n",
        "                            'opp_FiscalQuarter': 0, 'opp_CloseDate': '1900-01-01', 'opp_StageName': 'none', \n",
        "                            'opp_Contract_Length__c': 1, 'opp_Contract_Length_CPQ__c': 1, 'opp_Amount': 0, \n",
        "                            'opp_SBCF_Current_License_Qty__c': 0, 'opp_CreatedDate': '1900-01-01', 'opp_Opportunity_Name_Append__c': 'none', \n",
        "                            'opp_NewARR__c': 0, 'opp_RenewalARR__c': 0, 'opp_New_Bookings_Local_Currency__c': 0\n",
        "                         }\n",
        "            opportunities.fillna(value=missing_values, inplace=True)\n",
        "\n",
        "            # Convert to DateTime\n",
        "            opp_date_labels = ['opp_CloseDate', 'opp_CreatedDate', 'opp_License_Start_Date_New__c',  'opp_License_End_Date_New__c']\n",
        "            opportunities[opp_date_labels] = opportunities[opp_date_labels].apply(pd.to_datetime)\n",
        "            # maybe use this to convert to datetime instead: np.array(['2001-01-01T12:00', '2002-02-03T13:56:03.172'], dtype='datetime64')\n",
        "            \n",
        "            # Convert Opp ARR in INR currency to USD\n",
        "            opp_int_labels = ['opp_FiscalYear', 'opp_FiscalQuarter', 'opp_SBCF_Prior_License_Qty__c', 'opp_Contract_Length__c', 'opp_Contract_Length_CPQ__c']\n",
        "            opportunities[opp_int_labels] = opportunities[opp_int_labels].apply(pd.to_numeric)\n",
        "            \n",
        "            # Sort Opp Stages and Convert to Categorical: faster processing and indexing\n",
        "            categorical_labels = ['opp_StageName', 'opp_Type']\n",
        "            opportunities.opp_StageName = pd.Categorical(opportunities.opp_StageName, categories=sorted(list(opportunities.opp_StageName.unique())), ordered=True)\n",
        "            opportunities[categorical_labels] = opportunities[categorical_labels].apply(pd.Categorical)\n",
        "            \n",
        "            # Add new ARR fields for USD Converted ARR\n",
        "            opp_arr_labels = ['opp_NewARR__c', 'opp_RenewalARR__c', 'opp_New_Bookings_Local_Currency__c']\n",
        "            is_opp_CurrencyIsoCode_INR =  opportunities.loc[opportunities.opp_CurrencyIsoCode == 'INR'].index\n",
        "            for opp_arr_label in opp_arr_labels: \n",
        "                new_arr_label = '{}_Converted__USD'.format(str(opp_arr_label))\n",
        "                opportunities.loc[:, new_arr_label] = opportunities.loc[:, opp_arr_label]\n",
        "                opportunities.loc[idx[is_opp_CurrencyIsoCode_INR], new_arr_label] = (opportunities.loc[idx[is_opp_CurrencyIsoCode_INR], new_arr_label] * self.inr_to_usd)\n",
        "            \n",
        "                        \n",
        "            # New fields for Opp/Case Created/Closed Date [Year, Quarter, Week]\n",
        "            opportunities['opp_CreatedDate_Year'] = opportunities.opp_CreatedDate.dt.year\n",
        "            opportunities['opp_CreatedDate_Quarter'] = opportunities.opp_CreatedDate.dt.quarter\n",
        "            opportunities['opp_CreatedDate_Week'] = opportunities.opp_CreatedDate.dt.isocalendar().week\n",
        "            opportunities['opp_CloseDate_Year'] = opportunities.opp_CloseDate.dt.year\n",
        "            opportunities['opp_CloseDate_Quarter'] = opportunities.opp_CloseDate.dt.quarter\n",
        "            opportunities['opp_CloseDate_Week'] = opportunities.opp_CloseDate.dt.isocalendar().week\n",
        "            \n",
        "            # Convert to Int\n",
        "            opp_int_labels = ['opp_CreatedDate_Year', 'opp_CreatedDate_Quarter', 'opp_CreatedDate_Week', \n",
        "                              'opp_CloseDate_Year', 'opp_CloseDate_Quarter', 'opp_CloseDate_Week']\n",
        "            opportunities[opp_int_labels] = opportunities[opp_int_labels].apply(pd.to_numeric) \n",
        "            \n",
        "            # -*- ------------------- CASE DATA ------------------- -*-\n",
        "            \n",
        "            # Clean fields with NaN\n",
        "            sys.stdout.write('\\n\\n{0} Cleaning Cases Data {1}\\n'.format('\\x1b[6;30;43m', '\\x1b[0m'))\n",
        "            case_values = {\n",
        "                            'case_CreatedDate': '1900-01-01',\n",
        "                            'case_Creator_Role__c': 'None'\n",
        "                          }\n",
        "            cases.fillna(value=case_values, inplace=True)\n",
        "            \n",
        "            # Convert to DateTime\n",
        "            case_date_labels = ['case_CreatedDate']\n",
        "            cases[case_date_labels] = cases[case_date_labels].apply(pd.to_datetime)         \n",
        "         \n",
        "            # Convert Opp ARR in INR currency to USD\n",
        "            cases['case_CreatedYear'] = cases.case_CreatedDate.dt.year\n",
        "            cases['case_CreatedQuarter'] = cases.case_CreatedDate.dt.quarter\n",
        "            cases['case_CreatedWeek'] = cases.case_CreatedDate.dt.isocalendar().week\n",
        "            \n",
        "            # Convert to Int\n",
        "            case_int_labels = ['case_CaseNumber', 'case_CreatedYear', 'case_CreatedQuarter', 'case_CreatedWeek']\n",
        "            cases[case_int_labels] = cases[case_int_labels].apply(pd.to_numeric)  \n",
        "\n",
        "            # Create S&SE Case Creator Team Values\n",
        "            is_AE = cases.case_Creator_Role__c.str.contains('AE|SE|SAE|ADR|SDR|UFB Mgmt|Rev Ops|Deactivated')\n",
        "            is_CS = cases.case_Creator_Role__c.str.contains('CS|CSM|CSA|ADR|SDR')\n",
        "            is_NBV = cases.case_Creator_Role__c.str.contains('Udemy NV|UFG')\n",
        "            is_UG = cases.case_Creator_Role__c.str.contains('UFG')\n",
        "            is_BD = cases.case_Creator_Role__c.str.contains('Partnerships')\n",
        "            is_support = cases.case_Creator_Role__c.str.contains('Sales Customer Experience')\n",
        "\n",
        "            cases['case_Creator_Team'] = 'Sales'\n",
        "            cases.loc[idx[is_CS], 'case_Creator_Team'] = 'Customer Success'\n",
        "            cases.loc[idx[is_NBV], 'case_Creator_Team'] = 'NBV'\n",
        "            cases.loc[idx[is_UG], 'case_Creator_Team'] = 'UG'\n",
        "            cases.loc[idx[is_BD], 'case_Creator_Team'] = 'BD'\n",
        "            cases.loc[idx[is_support], 'case_Creator_Team'] = 'UB Support'\n",
        "            \n",
        "            # -*- ------------------- MERGE DATA ------------------- -*-\n",
        "            \n",
        "            '''\n",
        "            Create New aggs_ DataFrames:\n",
        "                1. account_opps = Accounts + Opportunities\n",
        "                2. account_opps_cases = account_opps + cases\n",
        "                3. case_opps = cases + account_opps\n",
        "            '''\n",
        "            \n",
        "            # 1. Merge Opps with Accounts (account_opps); remove dups, and drop the opp_Account data field (not needed after )\n",
        "            sys.stdout.write('\\n\\n{0} Merging Accounts, Opportunities, and Cases Records (acct_opps) {1}\\n'.format('\\x1b[6;30;43m', '\\x1b[0m'))\n",
        "            account_opps = pd.merge(opportunities, accounts, how='left', left_on='opp_AccountId', right_on='acct_Id')\n",
        "            account_opps.drop_duplicates(subset=['opp_Id'], inplace=True)\n",
        "            account_opps.drop(columns=['opp_Account'], axis=1, inplace=True) \n",
        "            \n",
        "            # 2. Merge account_opps with Cases; account_opps_cases\n",
        "            account_opps_cases = pd.merge(account_opps, \n",
        "                                               cases[['case_CreatedYear','case_CreatedQuarter', 'case_CreatedWeek', 'case_Id', \n",
        "                                                      'case_Related_Opportunity__c','case_CaseNumber', 'case_OwnerId', 'case_Status', \n",
        "                                                      'case_AE__c', 'case_Type', 'case_Sub_Type__c', 'case_Description', 'case_Subject',\n",
        "                                                      'case_Creator_Team', 'case_CreatedDate', 'case_Oppty_Stage__c', 'case_CreatedById', 'case_RecordTypeName__c']], \n",
        "                                               how='left', \n",
        "                                               left_on='opp_Id', \n",
        "                                               right_on='case_Related_Opportunity__c'\n",
        "                                              )\n",
        "            # 3. Merge User Name for case_OwnerId with account_opps_cases; drop dups, rename columns, and drop duplicate columns\n",
        "            account_opps_cases = pd.merge(account_opps_cases, users[['user_Id', 'user_Name']], how='left', left_on='case_OwnerId',  right_on='user_Id')\n",
        "            account_opps_cases.drop_duplicates(subset=['opp_Id', 'case_Id'], inplace=True)\n",
        "            account_opps_cases.rename(columns={'user_Name':'case_SSEOwnerName'}, inplace=True)\n",
        "            account_opps_cases.drop(columns=['user_Id'], axis=1, inplace=True)        \n",
        "\n",
        "            # 3. Merge User Name for case_CreatedById with account_opps_cases; drop dups, rename columns, and drop duplicate columns\n",
        "            account_opps_cases = pd.merge(account_opps_cases, users[['user_Id', 'user_Name']], how='left', left_on='case_CreatedById',  right_on='user_Id')\n",
        "            account_opps_cases.drop_duplicates(subset=['opp_Id', 'case_Id'], inplace=True)\n",
        "            account_opps_cases.rename(columns={'user_Name':'case_CreatedByName'}, inplace=True)\n",
        "            account_opps_cases.drop(columns=['user_Id'], axis=1, inplace=True)   \n",
        "\n",
        "            # 5. Merge User Name for opp_OwnerId, drop dups, and then rename columns\n",
        "            account_opps_cases = pd.merge(account_opps_cases, users[['user_Id', 'user_Name', 'user_Manager_for_Reports__c', 'user_Email']], how='left', left_on='opp_OwnerId',  right_on='user_Id')\n",
        "            account_opps_cases.drop_duplicates(subset=['opp_Id', 'case_Id'], inplace=True)\n",
        "            account_opps_cases.rename(columns={'user_Name': 'opp_OwnerName', 'user_Manager_for_Reports__c': 'opp_AE_Manager', 'user_Email': 'opp_OwnerEmail'}, inplace=True)\n",
        "            account_opps_cases.drop(columns=['user_Id'], axis=1, inplace=True) \n",
        "          \n",
        "            # 6. Merge IronClad data\n",
        "            account_opps_cases = pd.merge(account_opps_cases, iron_clad_workflows, how='left', left_on='case_Id',  right_on='ironclad_workflow_Case__c')\n",
        "            account_opps_cases.drop_duplicates(subset=['opp_Id', 'case_Id'], inplace=True)           \n",
        "            \n",
        "            # 7. Merge Opp Line Items (i.e., Product) and Product Details\n",
        "            sys.stdout.write('\\n\\n{0} Merging Opportunity Line Items (i.e., Products) with Line Item Product Details {1}\\n'.format('\\x1b[6;30;43m', '\\x1b[0m'))\n",
        "            temp_merge = pd.merge(opportunity_products, product_details, how='left', left_on='oppli_Product2Id', right_on='prod_Id')\n",
        "            temp_merge.drop_duplicates(subset=['oppli_Id'], inplace=True)\n",
        "            account_opps_cases = pd.merge(account_opps_cases, temp_merge, how='left', left_on='opp_Id',  right_on='oppli_OpportunityId')\n",
        "            account_opps_cases.drop_duplicates(subset=['opp_Id', 'case_Id'], inplace=True)\n",
        "            \n",
        "\n",
        "            # Get S&SE Assignment Rules and add Assigned S&SE attribute/column ('opp_SE_Assigned')\n",
        "            sse_assignments = self.sse_assignments\n",
        "            account_opps_cases['opp_SE_Assigned']='None'\n",
        "            for region in sse_assignments.keys():\n",
        "                if \"AMER\" in region: \n",
        "                    for sse in sse_assignments[region][0].keys():\n",
        "                        if 'Seth Klein' not in sse:\n",
        "                            criteria = account_opps_cases.opp_Live_UFB_Owner_String__c.str.contains(\"|\".join(sse_assignments[region][0][sse][0]['opp_Live_UFB_Owner_String__c']))\n",
        "                            idx_keys = account_opps_cases.loc[criteria].index\n",
        "                            account_opps_cases.loc[idx[idx_keys], 'opp_SE_Assigned'] = sse \n",
        "                            \n",
        "                if \"EMEA\" in region: \n",
        "                    for sse in sse_assignments[region][0].keys():\n",
        "                        if 'Garry Hickey' not in sse:\n",
        "                            criteria = ((account_opps_cases.acct_Sub_Region__c.str.contains(\"|\".join(sse_assignments[region][0][sse][0]['acct_Sub_Region__c']))) \n",
        "                                       & account_opps_cases.opp_Live_UFB_Owner_String__c.str.contains(\"|\".join(sse_assignments[region][0][sse][0]['opp_Live_UFB_Owner_String__c'])))\n",
        "                            idx_keys = account_opps_cases.loc[criteria].index\n",
        "                            account_opps_cases.loc[idx[idx_keys], 'opp_SE_Assigned'] = sse\n",
        "                if \"APAC\" in region: \n",
        "                    for sse in sse_assignments[region][0].keys():\n",
        "                        if 'Steven Reynolds' not in sse:\n",
        "                            criteria = ((account_opps_cases.acct_BillingCountry.str.contains(\"|\".join(sse_assignments[region][0][sse][0]['acct_BillingCountry']))) \n",
        "                                       & account_opps_cases.opp_Live_UFB_Owner_String__c.str.contains(\"|\".join(sse_assignments[region][0][sse][0]['opp_Live_UFB_Owner_String__c'])))\n",
        "                            idx_keys = account_opps_cases.loc[criteria].index\n",
        "                            account_opps_cases.loc[idx[idx_keys], 'opp_SE_Assigned'] = sse \n",
        "           \n",
        "            # Add Opportunity Contract Length\n",
        "            account_opps_cases['opp_Contract_Length_CPQ__c_Years'] = 0\n",
        "            account_opps_cases.loc[((account_opps_cases.opp_Contract_Length_CPQ__c >0) & (account_opps_cases.opp_Contract_Length_CPQ__c <=18)), ['opp_Contract_Length_CPQ__c_Years']] = 1\n",
        "            account_opps_cases.loc[((account_opps_cases.opp_Contract_Length_CPQ__c >18) & (account_opps_cases.opp_Contract_Length_CPQ__c <=30)), ['opp_Contract_Length_CPQ__c_Years']] = 2\n",
        "            account_opps_cases.loc[((account_opps_cases.opp_Contract_Length_CPQ__c >30) & (account_opps_cases.opp_Contract_Length_CPQ__c <=100)), ['opp_Contract_Length_CPQ__c_Years']] = 3\n",
        "            \n",
        "            # Clean Missing data\n",
        "            missing_values = {'opp_FiscalYear': 0, 'opp_FiscalQuarter': 0, 'case_CreatedYear': 0, 'case_CreatedQuarter': 0,\n",
        "                              'opp_SBCF_Prior_License_Qty__c': 0, 'case_CaseNumber': 0, 'case_CreatedWeek': 0, \n",
        "                              'case_SSEOwnerName': 'none', 'case_Status': 'none', \n",
        "                              'opp_ForecastCategory': 'none', 'prod_Name': 'none', 'opp_UFB_Sales_Owner_Segment__c': 'none',\n",
        "                              'prod_Family': 'none', 'oppli_ProductCode': 'none', 'acct_Customer_Tier__c': 'none', \n",
        "                              'opp_UFB_Sales_Owner_Team__c': 'none', 'opp_UFB_Sales_Owner_Region__c': 'none', \n",
        "                              'opp_UFB_Sales_Owner_Title__c': 'none', 'acct_CSM_Role__c': 'none', 'acct_Current_LMS__c': 'none',\n",
        "                              'case_Subject': 'none', 'acct_Region_Detail__c': 'none', 'acct_fcio1__Territory__c': 'none', 'acct_ZoomInfo_Industry__c': 'none',\n",
        "                              'case_Creator_Team': 'none', 'case_Oppty_Stage__c': 'none', 'acct_Partner_Type__c': 'none', 'case_Sub_Type__c': 'none'\n",
        "                             }\n",
        "            account_opps_cases.fillna(value=missing_values, inplace=True)       \n",
        "            \n",
        "            # Convert int64 to int32 Columns\n",
        "            cast_cols = dict()\n",
        "            cols = list()\n",
        "            cols = list(account_opps_cases.dtypes[account_opps_cases.dtypes == 'int64'].index)\n",
        "            for col in cols:\n",
        "                cast_cols[col] = 'int32'\n",
        "            account_opps_cases = account_opps_cases.astype(cast_cols)\n",
        "            \n",
        "            # Convert float64 to float32 Columns\n",
        "            cast_cols = dict()\n",
        "            cols = list()\n",
        "            cols = list(account_opps_cases.dtypes[account_opps_cases.dtypes == 'float64'].index)\n",
        "            for col in cols:\n",
        "                cast_cols[col] = 'float32'\n",
        "            account_opps_cases = account_opps_cases.astype(cast_cols) \n",
        "            \n",
        "            # Convert to Categorical Values: decreases DataFrame size!\n",
        "            categorical_labels = ['acct_Region__c', 'acct_Business_Segment__c', 'opp_Opportunity_Owner_Role__c', 'opp_Live_UFB_Owner_String__c', \n",
        "                                  'opp_OwnerEmail', 'opp_OwnerId', 'opp_RecordTypeId', 'opp_OwnerName', 'opp_Opportunity_Owner_ID__c', 'acct_Industry', \n",
        "                                  'acct_Account_Owner_Text__c', 'opp_Product_Type__c', 'opp_AE_Manager', 'acct_ProductType_f__c', 'opp_SE_Assigned', \n",
        "                                  'acct_Sub_Region__c', 'acct_BillingCountry', 'acct_Type', 'opp_ForecastCategoryName', 'opp_ForecastCategory', 'opp_CS_Segment__c', \n",
        "                                  'case_SSEOwnerName', 'acct_Region_text__c', 'case_Status', 'acct_Account_Record_Type_Name__c', 'prod_Name', 'opp_CurrencyIsoCode', \n",
        "                                  'acct_CurrencyIsoCode', 'opp_UFB_Sales_Owner_Segment__c', 'prod_Family', 'oppli_ProductCode', 'acct_Customer_Tier__c', \n",
        "                                  'opp_UFB_Sales_Owner_Team__c', 'opp_UFB_Sales_Owner_Region__c', 'opp_UFB_Sales_Owner_Title__c', 'acct_CSM_Role__c', \n",
        "                                  'acct_Current_LMS__c', 'case_Subject', 'acct_Region_Detail__c', 'acct_fcio1__Territory__c', 'acct_ZoomInfo_Industry__c', \n",
        "                                  'case_Creator_Team', 'case_Type', 'case_Sub_Type__c', 'case_Oppty_Stage__c', 'acct_Partner_Type__c']\n",
        "            \n",
        "            account_opps_cases[categorical_labels] = account_opps_cases[categorical_labels].apply(pd.Categorical)\n",
        "            \n",
        "            # Sort opp_CloseDate, Set Index (fast slicing), and Sort Columns labels (axis=1)\n",
        "            index = ['opp_FiscalYear', 'opp_FiscalQuarter', 'opp_StageName', 'opp_Type', \n",
        "                     'acct_Region_New__c', 'acct_Region__c', 'acct_Business_Segment__c',\n",
        "                     'case_CreatedYear','case_CreatedQuarter']\n",
        "            \n",
        "            # Cast DataFrame objects for better performance before indexing \n",
        "            cast_cols = {'opp_FiscalYear': 'int16', 'opp_FiscalYear': 'int16', 'case_CreatedYear': 'int16', 'case_CreatedQuarter': 'int16'}\n",
        "            account_opps_cases = account_opps_cases.astype(cast_cols)\n",
        "            \n",
        "            account_opps_cases.sort_values(by=['opp_CloseDate'], ascending=True, inplace=True)\n",
        "            account_opps_cases.set_index(index, inplace=True)\n",
        "            account_opps_cases.sort_index(axis=1, ascending=True, inplace=True)\n",
        "              \n",
        "            self.acct_opps_arr_columns = [col for col in account_opps_cases.columns if 'ARR' in col]\n",
        "            self.export_to_parquet(data=account_opps_cases, fname='acct_opps.parquet.gzip')\n",
        "            \n",
        "            sys.stdout.write('\\n{0} -------- DONE --------  {1}\\n'.format('\\x1b[32m', '\\x1b[32m'))\n",
        "            del accounts, opportunities, cases, iron_clad_workflows, users, opportunity_products, product_details, temp_merge\n",
        "            return account_opps_cases\n",
        "\n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error: <sf_Manager.get_clean_aggs__data()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def get_merged_opp_case_history_stages(self, left_data=pd.DataFrame(), right_data=pd.DataFrame(), right_by=''):\n",
        "\n",
        "        '''\n",
        "        Match Opportunity Stage cloases to the created Date of each Opportunity Case\n",
        "\n",
        "            Parameters:\n",
        "               1. left_data (Pandas DataFrame): DataFrame to Match with right; must contained a matching sorted index with right\n",
        "               2. right_data (Pandas DataFrame): DataFrame to Match with left; must contained a matching sorted index with left\n",
        "               3. right_by (string): Column to use as lookup from right DataFrame with Left DataFrame\n",
        "            \n",
        "            Returns:\n",
        "               1. matches (Pandas DataFrame): aggs_ with matches on OppStage Name at time Case was Created\n",
        "               Index(['oppfieldhist_CreatedById', 'oppfieldhist_CreatedDate', 'oppfieldhist_DataType', 'oppfieldhist_Field', 'oppfieldhist_Id', 'oppfieldhist_IsDeleted', 'opphist_StageName', 'oppfieldhist_OldValue', 'right_by']\n",
        "        '''       \n",
        "\n",
        "        try:\n",
        "            \n",
        "            # right = self.get_opportunity_history()\n",
        "\n",
        "            # Copy Opps with S&SE Cases to a new DataFrame; subset of required fields only\n",
        "            left = (left_data.loc[(left_data.case_Id.notnull()), \n",
        "                                  ['opp_Id', 'case_Id', 'case_CreatedDate']].reset_index(drop=True).copy())\n",
        "            \n",
        "            # Set Time Series index.  MUST be DateTime UTC, and MUST be Sorted; \n",
        "            left['case_CreatedDate'] = left['case_CreatedDate'].apply(pd.to_datetime)\n",
        "            left.sort_values(by=['case_CreatedDate'], ascending=[True], inplace=True)\n",
        "            left.set_index('case_CreatedDate', inplace=True)\n",
        "            \n",
        "            # Filter Opp History Records to match records of Opps with S&SE Cases\n",
        "            right = (right_data.loc[right_data[right_by].isin(left.opp_Id.unique()), :].copy())\n",
        "\n",
        "            # For roughly 10K records in both left and right DataFrames, it will take 5 minutes to match records based on closest TimeSeries index.\n",
        "            (matched_results, unmatched_results) = self.merge_asof(left=left, right=right, left_by=['opp_Id'], right_by=right_by)\n",
        "\n",
        "            # Merge matches\n",
        "            # [['opp_Id', 'case_Id'] + right_columns]\n",
        "            index_ = left_data.index\n",
        "            matches = (left_data.merge(matched_results, how='left', left_on=['opp_Id', 'case_Id'],  right_on=['opp_Id', 'case_Id'], suffixes=('_left', '_right')))\n",
        "            matches.set_index(index_, inplace=True)\n",
        "\n",
        "            # -*- Calculate S&SE New and Renewall ARR Impact & Merge with matches -*- \n",
        "            df = pd.DataFrame()\n",
        "            pvt = pd.DataFrame()\n",
        "            df = matches.loc[(matches.case_Id.notnull())].copy()\n",
        "            \n",
        "            # Better performance for getting/setting via sorted index\n",
        "            df.reset_index(inplace=True)\n",
        "            df.sort_index(inplace=True)\n",
        "\n",
        "            # -*- Get Aggregate Number of S&SE Cases, grouped by 'Opp_Id' 'case_OwnerId' (i.e., the S&SE Case Owner's SalesForce ID) -*-\n",
        "            values = ['case_Id']\n",
        "            pvt = pd.DataFrame()\n",
        "            pvt = (pd.pivot_table(data=df.reset_index(), \n",
        "                                   values=values, \n",
        "                                   index=['opp_Id', 'case_OwnerId'], \n",
        "                                   columns=None,\n",
        "                                   aggfunc={values[0]: np.count_nonzero}, \n",
        "                                   dropna=True,\n",
        "                                   fill_value=0\n",
        "                                  )\n",
        "                   )\n",
        "\n",
        "            opp_grp_total = pvt.groupby(level=[0]).transform('sum') # Group by Opp_Id\n",
        "            opp_sse_grp_total = pvt.groupby(level=[0, 1]).transform('sum') # Group by Opp_Id and case_OwnerId\n",
        "\n",
        "            # Merge Total No. of S&SE Cases for each Opp and each Opp and S&SE\n",
        "            aggs_ = pd.DataFrame()\n",
        "            aggs_ = (pd.merge(left=opp_grp_total, right=opp_sse_grp_total, how='left', left_index=True , right_index=True, suffixes=('_left', '_right')))\n",
        "            aggs_.rename(columns={'case_Id_left': 'case_SSE_Total_Cases', 'case_Id_right': 'case_SSE_Total_Opp_Cases'}, inplace=True)\n",
        "            aggs_['case_SSE_Percentage_Impact'] = aggs_.case_SSE_Total_Opp_Cases/aggs_.case_SSE_Total_Cases\n",
        "\n",
        "            # Merge with matches and calculate S&SE Revenue Impact for Opps engaged\n",
        "            matches = pd.merge(matches, aggs_, how='left', left_on=['opp_Id', 'case_OwnerId'], right_index=True,  suffixes=('_left', '_right'))\n",
        "            matches['case_SSE_NARR_Contribution'] = matches.opp_NewARR__c_Converted__USD * matches.case_SSE_Percentage_Impact\n",
        "            matches['case_SSE_RARR_Contribution'] = matches.opp_RenewalARR__c_Converted__USD * matches.case_SSE_Percentage_Impact\n",
        "            matches['opp_Weeks_to_Close'] = (matches.opp_CloseDate.dt.isocalendar().week.values) - (matches.opp_CreatedDate.dt.isocalendar().week.values)\n",
        "\n",
        "            # Align the Legacy Opp Stage Name Values with the New Stage Name Values\n",
        "            matches.loc[matches.opphist_StageName=='AE Qualified', 'opphist_StageName'] = '2-Qualified'\n",
        "            matches.loc[matches.opphist_StageName.isin(['Business Requirements Identified', 'Business requirements identifed']), 'opphist_StageName'] = '3-Business Requirements'\n",
        "            matches.loc[matches.opphist_StageName=='Finalizing Closure', 'opphist_StageName'] = '7-Finalizing Closure'\n",
        "            matches.loc[matches.opphist_StageName=='Finance Review', 'opphist_StageName'] = '8-Finance Review'\n",
        "            matches.loc[matches.opphist_StageName.isin(['Kick back to SDR/BDR', 'Lead from SDR/BDR']), 'opphist_StageName'] = '1-Pre-Qualified'\n",
        "            matches.loc[matches.opphist_StageName=='Negotiation/Procurement/Legal', 'opphist_StageName'] = '6-Negotiation'\n",
        "            matches.loc[matches.opphist_StageName=='Proof of Concept (POC)', 'opphist_StageName'] = '4-Solution Design'\n",
        "            matches.loc[matches.opphist_StageName=='Proposal/Price Presented', 'opphist_StageName'] = '5-Proposal'\n",
        "            matches.sort_index(axis=1, ascending=True, inplace=True)\n",
        "            \n",
        "            del left_data, right, right_data, df, pvt, opp_grp_total, opp_sse_grp_total, aggs_, matched_results, unmatched_results, index_\n",
        "            return matches\n",
        "        \n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error: <sf_Manager.get_aggs__opp_case_history_stages()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    def merge_asof(self, left=pd.DataFrame(), right=pd.DataFrame(), left_by=['case_CreatedDate'], right_by=['opphist_CreatedDate']):\n",
        "        '''\n",
        "        Match closest Opp History record by created Date with SalesForce Case by Created date, and by Opp Id. For large datasets (>100K records), consider refactoring this method with Dask DataFrames for parallelization\n",
        "\n",
        "            Parameters:\n",
        "                left (Pandas DataFrame object): SalesForce Records to match with right dataframe; must contain sorted TimeSeries field\n",
        "                right (Pandas DataFrame object): SalesForce Records to match with left; must contain roted TimeSeries index\n",
        "            \n",
        "            Returns:\n",
        "               1. matched_results (Pandas DataFrame): matched records from left DataFrame\n",
        "               2. unmatched_results (Pandas DataFrame): unmatched records from left DataFrame\n",
        "        '''       \n",
        "\n",
        "        try:\n",
        "            \n",
        "            # Initialize DataFrames for matches\n",
        "            matches = pd.DataFrame()\n",
        "            matched_results = pd.DataFrame()\n",
        "            unmatched_results = pd.DataFrame()\n",
        "            \n",
        "            # Results of the merge_asof matches\n",
        "            self.results = {'merge_asof_results_backward': set(), 'merge_asof_results_forward': set()}\n",
        "            \n",
        "            # Print Number of Records from Left (S&SE Cases) and Right (Opp History) DataFrames\n",
        "            sys.stdout.write('\\n{0} Records to Match {1} \\n\\n'.format('\\x1b[6;30;43m', '\\x1b[0m'))\n",
        "            sys.stdout.write('\\n{0}\\n'.format(left.info()))\n",
        "            sys.stdout.write('\\n\\n{0} Against Opp History Records {1} \\n\\n'.format('\\x1b[6;30;43m', '\\x1b[0m'))\n",
        "            sys.stdout.write('\\n{0}\\n'.format(right.info()))\n",
        "            \n",
        "            # Set initial merge_asof dearch direction\n",
        "            max_days = (left.index.max() - left.index.min()).days\n",
        "            max_tolerance = list(range(1, max_days))\n",
        "            tolerance = 1\n",
        "            search_direction = 'nearest'\n",
        "            sys.stdout.write('\\n{0} Cases Span Number of Days: {1} {2} \\n\\n'.format('\\x1b[6;30;43m', max_days, '\\x1b[0m'))\n",
        "            \n",
        "            # Iterate through number of days in the left (S&SE Cases) DataFrame\n",
        "            for tolerance in max_tolerance:\n",
        "                # Set search direction to 'backward' following initial 1 day 'nearest' matches; \n",
        "                if tolerance > 1:\n",
        "                    search_direction = 'backward'\n",
        "                matches = (pd.merge_asof(left, right, \n",
        "                                         left_index=True, right_index=True, \n",
        "                                         left_by=left_by, right_by=right_by, \n",
        "                                         tolerance=pd.Timedelta('{0}d'.format(str(tolerance))),\n",
        "                                         direction=search_direction, \n",
        "                                         allow_exact_matches=True, \n",
        "                                         suffixes=('_left', '_right'))\n",
        "                         )\n",
        "                \n",
        "                # Store matches\n",
        "                matched_results = pd.concat([matches.loc[matches[right_by].notnull(), :], matched_results], ignore_index=False, sort=False)\n",
        "                \n",
        "                # Remove matched records for next iteration;\n",
        "                left = matches[left.columns].loc[matches[right_by].isnull(), :].copy()\n",
        "                right = right[right.columns].loc[right[right_by].isin(left.opp_Id.unique()), :].copy() \n",
        "                matches = pd.DataFrame()\n",
        "                self.results['merge_asof_results_backward'].add('{0},{1},{2},{3}'.format(tolerance, len(matched_results), len(left), len(right)))\n",
        "                \n",
        "                # For unmatched records that remain, use the 'forward' search direction. \n",
        "                # Perhaps this should be implemented with remaining unmatched Cases but only after search using 'nearest' and 'backfward' is complete; This approach may match Opp History that was after the S&SE Case was created.\n",
        "                # Can implement this recursively later; lazy for now...\n",
        "               \n",
        "            if len(left) !=0:\n",
        "                max_days = (left.index.max() - left.index.min()).days\n",
        "                max_tolerance = list(range(1, max_days))\n",
        "                for tolerance in max_tolerance:\n",
        "                    search_direction = 'forward'\n",
        "                    matches = (pd.merge_asof(left, right, \n",
        "                                     left_index=True,  right_index=True, \n",
        "                                     left_by=left_by, right_by=right_by, \n",
        "                                     tolerance=pd.Timedelta('{0}d'.format(str(tolerance))), direction=search_direction, allow_exact_matches=True, \n",
        "                                     suffixes=('_left', '_right'))\n",
        "                     )\n",
        "\n",
        "                    # Store matches\n",
        "                    matched_results = pd.concat([matches.loc[matches[right_by].notnull(), :], matched_results], ignore_index=False, sort=False)\n",
        "                    # Remove matched records for next iteration\n",
        "                    left = matches[left.columns].loc[matches[right_by].isnull(), :].copy()\n",
        "                    right = right[right.columns].loc[right[right_by].isin(left.opp_Id.unique()), :].copy() \n",
        "                    matches = pd.DataFrame()\n",
        "                    self.results['merge_asof_results_forward'].add('{0},{1},{2},{3}'.format(tolerance, len(matched_results), len(left), len(right)))\n",
        "            \n",
        "                # For unmatched records that remain, use the 'backward' search direction. \n",
        "                # Perhaps this should be implemented with remaining unmatched Cases but only after search using 'nearest' and 'backfward' is complete; This approach may match Opp History that was after the S&SE Case was created.\n",
        "                # Can implement this recursively later; lazy for now...\n",
        "            \n",
        "            if len(left) !=0:\n",
        "                max_days = (left.index.max() - left.index.min()).days\n",
        "                max_tolerance = list(range(1, max_days))\n",
        "                for tolerance in max_tolerance:\n",
        "                    search_direction = 'backward'\n",
        "                    matches = (pd.merge_asof(left, right, \n",
        "                                     left_index=True,  right_index=True, \n",
        "                                     left_by=left_by, right_by=right_by, \n",
        "                                     tolerance=pd.Timedelta('{0}d'.format(str(tolerance))), direction=search_direction, allow_exact_matches=True, \n",
        "                                     suffixes=('_left', '_right'))\n",
        "                     )\n",
        "\n",
        "                    # Store matches\n",
        "                    matched_results = pd.concat([matches.loc[matches[right_by].notnull(), :], matched_results], ignore_index=False, sort=False)\n",
        "                    # Remove matched records for next iteration\n",
        "                    left = matches[left.columns].loc[matches[right_by].isnull(), :].copy()\n",
        "                    right = right[right.columns].loc[right[right_by].isin(left.opp_Id.unique()), :].copy() \n",
        "                    matches = pd.DataFrame()\n",
        "                    self.results['merge_asof_results_forward'].add('{0},{1},{2},{3}'.format(tolerance, len(matched_results), len(left), len(right)))\n",
        "\n",
        "            matched_results.sort_index(inplace=True)\n",
        "            unmatched_results = left.copy()\n",
        "            \n",
        "            sys.stdout.write('\\n\\n{0} matched_results {1} \\n\\n'.format('\\x1b[6;30;43m', '\\x1b[0m'))\n",
        "            sys.stdout.write('{0}'.format(matched_results.info()))\n",
        "            sys.stdout.write('\\n\\n{0} unmatched_results {1} \\n\\n'.format('\\x1b[6;30;43m', '\\x1b[0m'))\n",
        "            sys.stdout.write('{0}\\n\\n'.format(unmatched_results.info()))\n",
        "            \n",
        "            del left, right, matches\n",
        "            return (matched_results, unmatched_results)\n",
        "        \n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error: <sf_Manager.match_opp_stage_to_cases()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "    def export_to_csv(self, data, **kwargs):\n",
        "        \n",
        "        '''\n",
        "        Exports Pandas DataFrame to CSV:\n",
        "        \n",
        "        Parameters\n",
        "            data: Pandas DataFrame object to export\n",
        "            kwargs (dict{}): export parameters: {file_path: 'file_path', file_name: 'file_name', years: list(years)}\n",
        "        \n",
        "        Returns:\n",
        "            \n",
        "            Export Pandas DataFrame Object to CSV utf-8 encoded file\n",
        "        '''        \n",
        "        \n",
        "        try:\n",
        "            file_path = GOOGLE_PATH + 'Shareddrives/UB/Sales/Sales & Solution Engineers/Sales/Analysis/data/opps_cases/{0}.csv'.format(kwargs.get('file_path'), kwargs.get('file_name'))\n",
        "            sys.stdout.write('\\n{0} >> Exporting {1} {2}'.format('\\x1b[1;33;40m','\\x1b[0m',  file_path))\n",
        "            data.to_csv(path_or_buf=file_path, sep=',', encoding='utf-8')\n",
        "            \n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <sf_Manager.export_to_csv()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "    def export_to_parquet(self, data, fname='my-parquet-data_{0}.parquet.gzip'.format(pd.Timestamp(datetime.now()).value)):\n",
        "        \n",
        "        '''\n",
        "        Exports Pandas DataFrame to CSV:\n",
        "        \n",
        "        Parameters\n",
        "            data: Pandas DataFrame object to export\n",
        "            fname (string): file name to export\n",
        "        \n",
        "        Returns:\n",
        "            \n",
        "            Export Pandas DataFrame Object to CSV utf-8 encoded file\n",
        "        '''        \n",
        "        \n",
        "        try:\n",
        "        \n",
        "            # Export Data to Google Drive, as parquet GZip file\n",
        "            fpath = GOOGLE_PATH + 'Shareddrives/UB/Sales/Sales & Solution Engineers/Sales/Analysis/data/opps_cases/' + fname\n",
        "            data.to_parquet(path=fpath, index=True)\n",
        "            sys.stdout.write('\\n\\n -*- {0}Exported Parquet File: {2}{1} -*- \\n\\n'.format('\\x1b[6;30;46m','\\x1b[0m', fpath))\n",
        "            \n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <sf_Manager.export_to_parquet()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "            return pd.DataFrame()            \n",
        "\n",
        "    def read_from_parquet(self, fname='acct_opps.parquet.gzip'):\n",
        "        \n",
        "        '''\n",
        "        Import Parquet File into Pandas DataFrame to CSV:\n",
        "        \n",
        "        Parameters\n",
        "            fname (string): file name to import\n",
        "        \n",
        "        Returns:\n",
        "            Pandas DataFrame Object\n",
        "        '''        \n",
        "        try:\n",
        "            # Export Data to Google Drive, as parquet GZip file\n",
        "            fpath = GOOGLE_PATH + 'Shareddrives/UB/Sales/Sales & Solution Engineers/Sales/Analysis/data/' + fname\n",
        "            sys.stdout.write('\\n\\n -*- {0}Reading Parquet File: {2}{1} -*- \\n\\n'.format('\\x1b[6;30;46m','\\x1b[0m', fpath))\n",
        "            data = pd.read_parquet(path=fpath)\n",
        "            return data\n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <sf_Manager.read_from_parquet()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "            return pd.DataFrame()                  \n",
        "\n",
        "    def _open_json_file(self, path_or_buf=str(os.path.expanduser(\"~\"))):\n",
        "        \n",
        "        '''\n",
        "        Open json file and return results\n",
        "        \n",
        "        Parameters:\n",
        "            path_or_buf (string): Path to JSON File\n",
        "        \n",
        "        Returns:\n",
        "            JSON Dict\n",
        "        '''      \n",
        "        try:\n",
        "            with open(path_or_buf) as json_file:\n",
        "                data = json.load(json_file)\n",
        "            return data\n",
        "\n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <sf_Manager._open_json_file()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "            return 0\n",
        "    \n",
        "    def _mount_google_drive(self):\n",
        "        '''\n",
        "        Desscription: Mount Google Drive\n",
        "        \n",
        "        Parameters:\n",
        "            None\n",
        "        \n",
        "        Returns:\n",
        "            None\n",
        "        '''  \n",
        "        try:\n",
        "          # Mount Google Drive\n",
        "          drive.mount('/content/gdrive', force_remount=True)\n",
        "          # List files for specific Google Drive Directory\n",
        "          # !ls '/content/gdrive/MyDrive'   \n",
        "        \n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <sf_Manager.mount_google_drive()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "            return 0 \n",
        "         \n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    try:\n",
        "        \n",
        "        # Account fields to retrieve; DO NOT use Wilcard (*) with SalesForce SOQL Queries; it does not work; must specify fields in SOQL Query\n",
        "        acct_labels = 'Id, Name, Type, BillingCountry, Industry, AnnualRevenue, NumberOfEmployees, ' + \\\n",
        "        'CurrencyIsoCode, Account_Owner_Text__c, Total_Opportunities__c, No_of_Open_Optys__c, Region_New__c, ' + \\\n",
        "        'Region__c, Sub_Region__c, Region_Detail__c, Region_text__c, Vertical__c, Business_Segment__c, ZoomInfo_Industry__c, Earliest_Renewal_Date__c, CSM_Role__c, ' + \\\n",
        "        'ProductType_f__c, Current_LMS__c, Account_Record_Type_Name__c, Employee_Segment__c, Customer_Tier__c, LastActivityDate, fcio1__Territory__c, Partner_Type__c'\n",
        "        \n",
        "        # Opportunity fields to retrieve;\n",
        "        opp_labels = 'AccountId, FiscalQuarter, FiscalYear, Id, Name, CreatedDate, CloseDate, Type, ForecastCategory, ForecastCategoryName, CSM_CSA_Oppty_User_ID__c, '+\\\n",
        "        'StageName, Amount, Annual_Recurring_Revenue_ARR__c, RenewalARR__c, Delta_ARR_NEW__c, NewARR__c, New_Bookings_Local_Currency__c, SBCF_Prior_ARR__c, '+\\\n",
        "        'SBCF_Current_ARR__c, JWT_Gross_ARR_Reseller__c, Prior_ARR_Amendment__c, Prior_ARR_Override__c, Opportunity_Open_ARR__c, '+\\\n",
        "        'License_Start_Date_New__c, License_End_Date_New__c, License_Term__c, Contract_Length__c, Contract_Length_CPQ__c, Current_ARR_Override__c, '+\\\n",
        "        'Delta_ARR__c, SBCF_Current_License_Qty__c, SBCF_Prior_License_Qty__c, SBCF_Delta_License_Qty__c, Live_UFB_Owner_String__c, OwnerId, '+\\\n",
        "        'CS_Segment__c, Closed_Lost_Details__c, Closed_Lost_Reason__c, Lost_Reason__c, IsWon, Probability, CurrencyIsoCode, CorpU__c, '+\\\n",
        "        'Opportunity_Name_Append__c, Next_Steps_UFB__c, Recommended_Next_Steps__c, Next_Step_Last_Updated__c, RecordTypeId, NextStep, '+\\\n",
        "        'Current_Content_Provider__c, Dc_Business_Commercial__c, Dc_Decision_Criteria_Technical__c, Dp_Business_Decision_Making__c, Qualified_to_Closed_Days__c, '+\\\n",
        "        'Dp_Technical_Decision_Making__c, E_Economic_Buyer__c, Exec_Sponsor_Engagement__c, Executive_Sponsor__c, I_Indentify_pain__c, '+\\\n",
        "        'I_Identify_Pain_notes__c, M_Metrics_Desired_Outcomes_Notes__c, P_Paper_Process__c, Opportunity_Owner_ID__c, Opportunity_Owner_Role__c, Opportunity_Owner_Account_Owner_Match__c, '+\\\n",
        "        'UFB_Sales_Owner_String_Stamp__c, UFB_Sales_Owner_Segment__c, UFB_Sales_Owner_Region__c, UFB_Sales_Owner_Title__c, UFB_Sales_Owner_Team__c, Product_Type__c, Product_Identification_Pipeline__c'\n",
        "           \n",
        "        # Get SalesForce API handle\n",
        "        sf = sf_Manager(acct_labels=acct_labels, opp_labels=opp_labels) \n",
        "    except Exception as error:\n",
        "        sys.stdout.write('\\n{0} **** Error in <sf_manager()__main__> {2} ***** {1} \\n\\n {3}'.format('\\x1b[0;31;40m', '\\x1b[0m', str(error), traceback.format_exc()))      "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get API Data - Clean and Merged"
      ],
      "metadata": {
        "id": "El5hVHzTbRYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "%%capture\n",
        "# Get Accounts and Opps from SFDC API\n",
        "years = [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026]\n",
        "accounts, opportunities = sf.get_accounts_opportunities(years=years)\n",
        "opportunities_oringinal_rows = opportunities.shape[0]\n",
        "opportunities.dropna(subset=['opp_Account'],inplace=True)\n",
        "(sys.stdout.write('\\nDropped {0} records with null opp_Account entries\\n'\n",
        "                  .format(opportunities_oringinal_rows - opportunities.shape[0])))\n",
        "\n",
        "# Get S&SE Cases\n",
        "soql_query = \"SELECT \" + sf.case_fields + \\\n",
        "             \" FROM Case WHERE OwnerId IN (\" + sf.SSEIDs + \\\n",
        "             \") AND RecordTypeName__c IN ('UFB SE Request', 'UFB Rev Ops', 'UB Request for Proposal (RFP)')\"\n",
        "cases = sf.get_cases(soql_query=soql_query)\n",
        "cases_oringinal_rows = cases.shape[0]\n",
        "cases.dropna(subset=['case_Related_Opportunity__c'], inplace=True)\n",
        "sys.stdout.write('\\nDropped {0} records with null case_Related_Opportunity__c entries\\n'.format(cases_oringinal_rows - cases.shape[0]))\n",
        "\n",
        "(sys.stdout.write('\\nData Stats:\\n\\n{0}>>accounts: {1}\\n>>opportunities:{2}\\n>>cases:{3}{4}\\n'\n",
        "                  .format('\\x1b[5;30;46m', len(accounts), len(opportunities), len(cases), '\\x1b[0m')))\n",
        "\n",
        "(sys.stdout.write('\\n-----{0} Merging Accounts, Opportunities and Cases data  {1}-----\\n'\n",
        "                  .format('\\x1b[6;30;43m', '\\x1b[0m')))\n",
        "\n",
        "# Merge Data\n",
        "acct_opps = sf.get_clean_merged_data(accounts.copy(), opportunities.copy(), cases.copy())\n",
        "acct_opps.sort_index(axis=0, sort_remaining=True, inplace=True)\n",
        "(sys.stdout.write('\\n\\n-----{0} Is Acct Opps Index Sorted? {1} {2}-----\\n\\n'\n",
        "                  .format('\\x1b[6;30;43m', acct_opps.index.is_monotonic_increasing, '\\x1b[0m')))"
      ],
      "metadata": {
        "id": "ADJGSDgXMMVx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "261c3380-91ae-443c-fc8d-8ac7565aa758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "          <style>\n",
              "            pre {\n",
              "                white-space: pre-wrap;\n",
              "            }\n",
              "          </style>\n",
              "          "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 41s, sys: 17.4 s, total: 2min 58s\n",
            "Wall time: 13min 13s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Get Data - Parquet File [GDrive]"
      ],
      "metadata": {
        "id": "KUDLmtmlXibD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get acct_opps and case_opps Data from GDrive Parquet file\n",
        "acct_opps = sf.read_from_parquet(fname='acct_opps.parquet.gzip')\n",
        "# case_opps = (acct_opps.loc[(acct_opps.case_Id.notnull()), ['opp_Id', 'case_Id', 'case_CreatedDate']].copy())"
      ],
      "metadata": {
        "id": "U9Llqk5xXUI_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "23bcbc6b-6217-49d5-dd8e-aca2912a674a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "          <style>\n",
              "            pre {\n",
              "                white-space: pre-wrap;\n",
              "            }\n",
              "          </style>\n",
              "          "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " -*- \u001b[6;30;46mReading Parquet File: /content/gdrive/Shareddrives/UB/Sales/Sales & Solution Engineers/Sales/Analysis/data/acct_opps.parquet.gzip\u001b[0m -*- \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acct_opps[acct_opps.opp_Id=='0061T00000wNNLBQA4'].T"
      ],
      "metadata": {
        "id": "jOjXrpFhXurc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Get Opp History Merge"
      ],
      "metadata": {
        "id": "yat6fvUb4o97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Accout and Opps with S&SE Case Opps Only\n",
        "left=acct_opps.loc[acct_opps.case_Id.notnull()].copy()\n",
        "# Pull Opportunity History from SFDC API\n",
        "# opp_hist=sf.get_opportunity_history()\n",
        "# Pull Opportunity History from Google Drive Parquet File\n",
        "opp_hist=sf.read_from_parquet(fname='opp_history.parquet.gzip')\n",
        "right=opp_hist[['opphist_OpportunityId', 'opphist_CreatedDate_', 'opphist_StageName', 'opphist_ExpectedRevenue']].copy()\n",
        "right=right.loc[right.opphist_OpportunityId.isin(acct_opps.opp_Id.unique())].copy()\n",
        "\n",
        "acct_opps_temp=(sf.get_merged_opp_case_history_stages(\n",
        "    left_data=left.copy(), \n",
        "    right_data=right.copy(), \n",
        "    right_by='opphist_OpportunityId'))\n",
        "\n",
        "# Clean fields with NaN\n",
        "missing_values = {\n",
        "                'opphist_ExpectedRevenue': 0,\n",
        "                'opphist_StageName': 'none'\n",
        "              }\n",
        "acct_opps_temp.fillna(value=missing_values, inplace=True)\n",
        "# Set Categorical labels for better performance\n",
        "categorical_labels = ['opphist_StageName']\n",
        "acct_opps_temp[categorical_labels] = acct_opps_temp[categorical_labels].apply(pd.Categorical)\n",
        "# Cast DataFrame objects for better performance before indexing \n",
        "cast_cols = {'opphist_ExpectedRevenue': 'float32'}\n",
        "acct_opps_temp = acct_opps_temp.astype(cast_cols)\n",
        "\n",
        "# -*- Merge with account_opps -*-\n",
        "acct_opps_merged = acct_opps.merge(acct_opps_temp[['opphist_CreatedDate_', 'opphist_StageName', 'opphist_ExpectedRevenue']], how='left', left_index=True, right_index=True)\n",
        "acct_opps_merged.drop_duplicates(subset=['opp_Id', 'case_Id'], inplace=True)\n",
        "acct_opps_merged.sort_index(axis=1, ascending=True, inplace=True)\n",
        "\n",
        "del acct_opps\n",
        "acct_opps = acct_opps_merged.copy()\n",
        "sf.export_to_parquet(data=acct_opps_merged, fname='acct_opps.parquet.gzip')\n",
        "del acct_opps_merged, acct_opps_temp, opp_hist, left, right"
      ],
      "metadata": {
        "id": "vmOd85ds4pMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acct_opps[['opp_Id', 'case_Id', 'case_CreatedDate', 'opphist_CreatedDate_', 'opphist_StageName', 'opphist_ExpectedRevenue']].loc[acct_opps.opp_Id=='0061T000015aml1QAA']"
      ],
      "metadata": {
        "id": "e_gDXbLcKepC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Analysis"
      ],
      "metadata": {
        "id": "4SlRUrWEbN8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Data_Analysis():\n",
        "    \n",
        "    def __init__(self, **kwargs):\n",
        "        \n",
        "        '''\n",
        "        Description: Various Analysis methods of SalesForce Accounts, Opportunities, and related Cases. \n",
        "        The outputs are mostly Pandas DataFrame Objects, such as combined PivotTables, GroupBy DataFrames and such. \n",
        "        It also includes key outputs on the S&SE Impact to New Business and Renewal Business, plus other Utilities for Displaying the information via Jupyter or Colab. \n",
        "        The methods defined here can all be translated to a Mode Dashboard with the corresponding SQL (or native Python on Mode) providing the same Analytics and Output, but more Scalable.\n",
        "        \n",
        "            Parameters:\n",
        "                kwargs:  parameters to apply filters on Data \n",
        "                    *acct_opps (Pandas DataFrame object): aggs_ Account and Opportunities DataFrames\n",
        "                    *case_opps (Pandas DataFrame object): aggs_ Account, Opportunities and Cases data\n",
        "                    *is_opp_FiscalYears list([int]): Opp Fiscal Year values to filter DataFrames\n",
        "                    *is_opp_FiscalQuarters list([int]): Opp Fiscal Year values to filter DataFrames\n",
        "        '''  \n",
        "        \n",
        "        # Select Data as Pandas DataFrame Objects\n",
        "        self.kwargs = kwargs\n",
        "        self.acct_opps = self.kwargs.get('acct_opps') # Pandas DataFrame Object: Salesforce Accounts and Opportunity Records\n",
        "        # self.case_opps = self.kwargs.get('case_opps') # Pandas DataFrame Object: SalesForce Cases\n",
        "        self.groupby_levels = self.kwargs.get('groupby_levels') # Pandas DataFrame Object: SalesForce Cases\n",
        "        \n",
        "        # SalesForce Data Dictionary\n",
        "        self.data_dictionary = {'opp_ForecastCategories': \n",
        "                                [\n",
        "                                     {'commit': 'Most likely/Probable', \n",
        "                                      'best case': 'optimistic', \n",
        "                                      'pipeline': 'total opportunity value open in SFDC'\n",
        "                                     }\n",
        "                                 ]\n",
        "                                }\n",
        "        \n",
        "        self.agg_dictionary = {'index_': list(self.acct_opps.index.names),\n",
        "                               'acct_fields': ['acct_Id', 'acct_Employee_Segment__c', 'acct_Account_Owner_Text__c'],\n",
        "                               'opp_fields': ['opp_Id', 'opp_FiscalYear', 'opp_StageName', 'opp_ForecastCategory', 'opp_Closed_Lost_Details__c', 'opp_Lost_Reason__c', \n",
        "                                              'opp_Closed_Lost_Reason__c', 'opp_Closed_Lost_Details__c', 'opphist_StageName', 'opp_SE_Assigned', 'opp_Contract_Length_CPQ__c_Years'],\n",
        "                               'opp_arr_fields': ['opp_NewARR__c_Converted__USD', 'opp_RenewalARR__c_Converted__USD', 'case_SSE_Percentage_Impact', \n",
        "                                                  'opp_NewARR__c_Converted__USD_SSE_IMPACT', 'opp_RenewalARR__c_Converted__USD_SSE_IMPACT'],\n",
        "                               'case_fields': ['case_Id', 'case_SSEOwnerName', 'case_Creator_Team', 'case_Type', 'case_Sub_Type__c']\n",
        "                               }\n",
        "\n",
        "        # Initialize default filter parameters\n",
        "        self.is_opp_FiscalYears = self.kwargs.get('is_opp_FiscalYears')\n",
        "        self.is_opp_FiscalQuarters = self.kwargs.get('is_opp_FiscalQuarters')\n",
        "        self.is_acct_Region_New__c = self.kwargs.get('is_acct_Region_New__c')\n",
        "        self.current_year = [pd.Timestamp(datetime.now()).year]\n",
        "        self.current_quarter = [pd.Timestamp(datetime.now()).quarter]\n",
        "        self.is_opp_Stages =list(self.acct_opps.index.get_level_values(list(self.acct_opps.index.names).index('opp_StageName')).unique())\n",
        "        self.opp_Stages_Ordered= sorted(list(self.acct_opps.index.get_level_values(list(self.acct_opps.index.names).index('opp_StageName')).unique()))\n",
        "        self.is_opp_Stages_Open = list([element for element in sorted(list(self.acct_opps.index.get_level_values(2).unique())) if element not in ['Closed Won', 'Closed Lost', 'Closed']])\n",
        "        self.is_opp_Types = ['New Business', 'Upsell', 'Upgrade', 'Expansion', 'Renewal with Upsell', 'Renewal']\n",
        "        \n",
        "        # Color Range for Charts: using Seaborn libary\n",
        "        self.cm = sns.color_palette(\"viridis\", as_cmap=True)\n",
        "        \n",
        "        # Store DataFrames in Dictionary\n",
        "        self._data_ = {\n",
        "                            'opps_closed_won_lost': self.get_opps_closed_won_lost(), \n",
        "                            'sse_rev_impact': self.get_sse_rev_impact(), \n",
        "                            'sse_rev_impact_analysis': self.get_sse_rev_impact_analysis()[0],\n",
        "                            'sse_rev_impact_analysis_styled': self.get_sse_rev_impact_analysis()[1],\n",
        "                            'get_opp_narr_stats': self.get_opp_narr_stats(),\n",
        "                            'get_corpU_opp_stats': self.get_corpU_opp_stats(),\n",
        "                            'team_plan_opps_won_styled': self.get_team_plan_opps_won()\n",
        "                    }\n",
        "        \n",
        "    def get_opp_narr_stats(self, is_years=[pd.Timestamp(datetime.now()).year], is_quarters=[pd.Timestamp(datetime.now()).quarter], \n",
        "                           group_levels=[0, 1], is_opp_region=slice(None), is_opp_type=['New Business', 'Upsell', 'Upgrade', 'Expansion', 'Renewal with Upsell', 'Clawback'], \n",
        "                           is_opp_stages=['Closed Won'], is_acct_record_type=['UFB'], is_opp_product_type=['Team Plan'], is_gte_arr=0):\n",
        "        '''\n",
        "        Returns a Pandas GroupBy DataFrame object for Account Opps, with S&SE Cases'\n",
        "\n",
        "        Parameters:\n",
        "            \n",
        "            is_years(list([int])): Opp Fiscal Years to filter data in DataFrame; defaults to current Year\n",
        "            is_quarters(list([int])): Opp Fiscal Quarters to filter data in DataFrame; defaults to current Quarter\n",
        "            is_opp_region(list([str])): Opp Regions to filter data in DataFrame; defaults to None\n",
        "            is_opp_type(list([str])): Opp Type to filter data in DataFrame; defaults to None\n",
        "            is_opp_stages(list([str])): Opp Stages to filter data in DataFrame;  defaults to None\n",
        "            is_acct_record_type(list([str])): Opp Type to filter data in DataFrame; defaults to None\n",
        "            is_opp_product_type(list([str])): Opp Type to filter data in DataFrame; defaults to None\n",
        "            is_gte_arr(int): Opp New ARR filter; default to zero (0)\n",
        "\n",
        "        Returns: \n",
        "                df_grp (Pandas GroupBy DataFrame): GroupBy DataFrame of New ARR Opps (i.e., acct_opps) and Contract Length Bins\n",
        "        '''\n",
        "        \n",
        "        try:\n",
        "            \n",
        "            # Get Data\n",
        "            df = pd.DataFrame()\n",
        "            df = self.acct_opps.loc[idx[is_years, is_quarters, is_opp_stages, is_opp_type, is_opp_region, :, :, :, :], :].copy()\n",
        "            df = (df.loc[(df.acct_Account_Record_Type_Name__c.isin(is_acct_record_type))\n",
        "                         & (~df.opp_Product_Type__c.isin(is_opp_product_type)) \n",
        "                         & (~df.opp_Opportunity_Owner_Role__c.isin(['UFB Sales Team Plan - Mgr'])) # Exclude Team Plan Opps\n",
        "                         # & (df.opp_NewARR__c_Converted__USD >= is_gte_arr)\n",
        "                        ].copy()\n",
        "                 )\n",
        "            \n",
        "            # Drop Duplicate Opps; created when merging Cases with Opps. \n",
        "            df.drop_duplicates(subset=['opp_Id', 'opp_NewARR__c_Converted__USD', 'opp_RenewalARR__c_Converted__USD'], inplace=True)\n",
        "            df.dropna(subset=['case_Id']) # Also Drop records with no S&SE Cases; i.e., case_id null/nan\n",
        "            df.sort_index(inplace=True) # Better Pandas performace\n",
        "\n",
        "            # Build opp_NewARR Bins: there may be a better way. . . lazy approach for now\n",
        "            bins = ['opp_ARR_lt_50K', 'opp_ARR_gt50K_lt100K', 'opp_ARR_gte_100K', 'opp_ARR_gte_200K', 'opp_ARR_gte_300K', 'opp_ARR_gte_400K',  'opp_ARR_gte_500K', \n",
        "                    'opp_Contract_Length_CPQ__c_Year1', 'opp_Contract_Length_CPQ__c_Year2', 'opp_Contract_Length_CPQ__c_Year3']\n",
        "            df[bins] = 0\n",
        "            # These are the filters for each bin\n",
        "            is_lt_50K = (df.opp_NewARR__c_Converted__USD <= 50000)\n",
        "            is_btw_50_100K = ((df.opp_NewARR__c_Converted__USD > 50000) & (df.opp_NewARR__c_Converted__USD < 100000))\n",
        "            is_gte_100K = df.opp_NewARR__c_Converted__USD >= 100000\n",
        "            is_gte_200K = df.opp_NewARR__c_Converted__USD >= 200000\n",
        "            is_gte_300K = df.opp_NewARR__c_Converted__USD >= 300000\n",
        "            is_gte_400K = df.opp_NewARR__c_Converted__USD >= 400000\n",
        "            is_gte_500K = df.opp_NewARR__c_Converted__USD >= 500000\n",
        "\n",
        "            # Assign NARR to Bins\n",
        "            df.loc[is_lt_50K, ['{0}'.format(bins[0])]] = df.opp_NewARR__c_Converted__USD.loc[is_lt_50K]\n",
        "            df.loc[is_btw_50_100K, ['{0}'.format(bins[1])]] = df.opp_NewARR__c_Converted__USD.loc[is_btw_50_100K]\n",
        "            df.loc[is_gte_100K, ['{0}'.format(bins[2])]] = df.opp_NewARR__c_Converted__USD.loc[is_gte_100K]\n",
        "            df.loc[is_gte_200K, ['{0}'.format(bins[3])]] = df.opp_NewARR__c_Converted__USD.loc[is_gte_200K]\n",
        "            df.loc[is_gte_300K, ['{0}'.format(bins[4])]] = df.opp_NewARR__c_Converted__USD.loc[is_gte_300K]\n",
        "            df.loc[is_gte_400K, ['{0}'.format(bins[5])]] = df.opp_NewARR__c_Converted__USD.loc[is_gte_400K]\n",
        "            df.loc[is_gte_500K, ['{0}'.format(bins[6])]] = df.opp_NewARR__c_Converted__USD.loc[is_gte_500K]\n",
        "\n",
        "            # Add Contract Length Bins                   \n",
        "            df.loc[((df.opp_Contract_Length_CPQ__c >0) & (df.opp_Contract_Length_CPQ__c <=18)), ['{0}'.format(bins[7])]] = 1\n",
        "            df.loc[((df.opp_Contract_Length_CPQ__c >18) & (df.opp_Contract_Length_CPQ__c <=30)), ['{0}'.format(bins[8])]] = 2\n",
        "            df.loc[((df.opp_Contract_Length_CPQ__c >30) & (df.opp_Contract_Length_CPQ__c <=100)), ['{0}'.format(bins[9])]] = 3\n",
        "\n",
        "            # -*- Define Pandas GroupBy Aggs -*-\n",
        "            aggs = {'{0}'.format(bins[0]): [np.sum],\n",
        "                    '{0}'.format(bins[1]): [np.sum],\n",
        "                    '{0}'.format(bins[2]): [np.sum],\n",
        "                    '{0}'.format(bins[3]): [np.sum],\n",
        "                    '{0}'.format(bins[4]): [np.sum],\n",
        "                    '{0}'.format(bins[5]): [np.sum],\n",
        "                    '{0}'.format(bins[6]): [np.sum],\n",
        "                    'opp_NewARR__c_Converted__USD': [np.sum],\n",
        "                    '{0}'.format(bins[7]): [np.count_nonzero],\n",
        "                    '{0}'.format(bins[8]): [np.count_nonzero],\n",
        "                    '{0}'.format(bins[9]): [np.count_nonzero],\n",
        "                   }\n",
        "            group = (df.groupby(level=[0, 1]).agg(aggs))\n",
        "\n",
        "            # Flatten GroupBy MultiIndex Column\n",
        "            group.columns = ['_'.join(col).strip() for col in group.columns.values]\n",
        "            df.reset_index(inplace=True)\n",
        "            caption=('Year(s): {0}\\nQuarter(s): {1}:\\nOpp Stage(s):{2}\\nOpp Type(s): {3}\\nNo of Opps: {4}\\nTotal New ARR: ${5:,.0f}'\n",
        "                     .format(sorted(df.opp_FiscalYear.unique()), sorted(df.opp_FiscalQuarter.unique()), sorted(df.opp_StageName.unique()), \n",
        "                             sorted(df.opp_Type.unique()), len(df.opp_Id.unique()), df.drop_duplicates(subset=['opp_Id']).opp_NewARR__c_Converted__USD.sum()\n",
        "                            )\n",
        "                    )\n",
        "            del df\n",
        "            return group\n",
        "        \n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <Data_Analysis.get_opp_narr_stats()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "            return pd.DataFrame()\n",
        "            \n",
        "    def get_opps_closed_won_lost(self):\n",
        "        '''\n",
        "        Returns Pandas DataFrame object of SalesForce Opportunities Won and Lost\n",
        "        \n",
        "        Parameters:\n",
        "            None\n",
        "            \n",
        "        Returns: \n",
        "            DataFrame object of SalesForce Opportunities Won and Lost\n",
        "        '''\n",
        "        try:\n",
        "            # -*- Get Opps WON -*-\n",
        "  \n",
        "            df = (self.acct_opps.loc[idx[self.is_opp_FiscalYears, \n",
        "                                         self.is_opp_FiscalQuarters, \n",
        "                                         ['Closed Won'], \n",
        "                                         self.is_opp_Types, \n",
        "                                         self.is_acct_Region_New__c, \n",
        "                                         :, \n",
        "                                         :, \n",
        "                                         :, \n",
        "                                         :], \n",
        "                                     :]\n",
        "                  .copy()\n",
        "                  )\n",
        "            \n",
        "            # Only UB Opportunities; Exclude NV and Team Plan Opportunities\n",
        "            df = (df.loc[(df.acct_Account_Record_Type_Name__c.isin(['UFB', 'UFG'])) \n",
        "                         & (~df.opp_Product_Type__c.isin(['Team Plan'])) \n",
        "                         & (~df.opp_Opportunity_Owner_Role__c.isin(['UFB Sales Team Plan - Mgr']))\n",
        "                         ].copy()\n",
        "                  )\n",
        "            \n",
        "            # Faster get/setting via sorted index\n",
        "            df.sort_index(inplace=True)\n",
        "            df.drop_duplicates(subset=['opp_Id'], inplace=True)\n",
        "            \n",
        "            # Groupby DataFrame\n",
        "            group1 = df.groupby(level=self.groupby_levels).agg(agg1=('opp_Id', 'nunique'),\n",
        "                                                               agg2=('opp_NewARR__c_Converted__USD', np.sum),\n",
        "                                                               agg3=('opp_RenewalARR__c_Converted__USD', np.sum)\n",
        "                                                              )\n",
        "\n",
        "            # -*- Get Opps LOST -*-\n",
        "            del df\n",
        "            df = (self.acct_opps.loc[idx[self.is_opp_FiscalYears, \n",
        "                                         self.is_opp_FiscalQuarters, \n",
        "                                         ['Closed Lost'], \n",
        "                                         self.is_opp_Types, \n",
        "                                         self.is_acct_Region_New__c, \n",
        "                                         :, \n",
        "                                         :, \n",
        "                                         :, \n",
        "                                         :], \n",
        "                                     :]\n",
        "                  .copy()\n",
        "                  )\n",
        "            df = (df.loc[(df.acct_Account_Record_Type_Name__c.isin(['UFB', 'UFG'])) \n",
        "                          & (~df.opp_Product_Type__c.isin(['Team Plan'])) \n",
        "                          & (~df.opp_Opportunity_Owner_Role__c.isin(['UFB Sales Team Plan - Mgr']))\n",
        "                          ].copy()\n",
        "                  )\n",
        "\n",
        "            # Better performance for getting/setting via sorted index\n",
        "            df.sort_index(inplace=True)\n",
        "            df.drop_duplicates(subset=['opp_Id'], inplace=True)\n",
        "\n",
        "            # Groupby DataFrame\n",
        "            group2 = (df.groupby(level=self.groupby_levels).agg(agg1_=('opp_Id', 'nunique'),\n",
        "                                                                agg2_=('opp_NewARR__c_Converted__USD', np.sum),\n",
        "                                                                agg3_=('opp_RenewalARR__c_Converted__USD', np.sum)\n",
        "                                                               )\n",
        "                      )\n",
        "            \n",
        "            group1.sort_index(axis=0, ascending=True, inplace=True)\n",
        "            group2.sort_index(axis=0, ascending=True, inplace=True)\n",
        "            aggs_ = pd.merge(group1, group2[['agg1_', 'agg2_', 'agg3_']], left_index=True, right_index=True, copy=True)\n",
        "            rename_mapper = {'agg1': 'no_of_opps_won', 'agg2': 'NARR_C', 'agg3': 'RARR_C', \n",
        "                             'agg1_': 'no_of_opps_lost', 'agg2_': 'NARR_C_Lost', 'agg3_': 'RARR_C_Lost'}    \n",
        "            aggs_.rename(columns=rename_mapper, inplace=True)\n",
        "            \n",
        "            del df, group1, group2\n",
        "            return aggs_\n",
        "        \n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <Data_Analysis.get_opps_closed_won_lost()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "            return pd.DataFrame() \n",
        "        \n",
        "    def get_team_plan_opps_won(self):\n",
        "        '''\n",
        "        Returns Pandas DataFrame GroupBy Object of Closed Won Team Plan Opportunities\n",
        "        \n",
        "        Parameters:\n",
        "            None\n",
        "        Returns\n",
        "            df_grp: Pandas DataFrame GroupBy Object of Closed Won Team Plan Opportunities\n",
        "        '''\n",
        "        #-*-Team Plan Opps Closed Won-*-\n",
        "        try:\n",
        "            # Select only Team Plan Opps Won\n",
        "            df = self.acct_opps.loc[idx[self.is_opp_FiscalYears, self.is_opp_FiscalQuarters, ['Closed Won'], :, self.is_acct_Region_New__c, :, :, :, :], :].copy()\n",
        "            df = (df.loc[(df.acct_Account_Record_Type_Name__c.isin(['UFB', 'UFG'])) \n",
        "                         & (df.opp_Product_Type__c.isin(['Team Plan'])) \n",
        "                         & (df.opp_Opportunity_Owner_Role__c.isin(['UFB Sales Team Plan - Mgr']))].copy()\n",
        "                 )\n",
        "           \n",
        "            # Faster get/setting via sorted index\n",
        "            df.sort_index(inplace=True)\n",
        "            df.drop_duplicates(subset=['opp_Id'], inplace=True)\n",
        "\n",
        "            # Group by OppFiscalYear and OppFiscalQuarter (level's 0 and 1 of the acct_opps data) DataFrame \n",
        "            df_grp = df.groupby(level=self.groupby_levels).agg(agg1=('opp_NewARR__c_Converted__USD', np.sum))\n",
        "            # Flatten Groupby DataFrame                                    \n",
        "            df_grp.sort_index(axis=0, ascending=True, inplace=True)           \n",
        "            df_rename_mapper = {\n",
        "                                'agg1': 'TP_NARR_C'\n",
        "            }          \n",
        "            df_grp.rename(columns=df_rename_mapper, inplace=True)\n",
        "            \n",
        "            del df\n",
        "            return df_grp\n",
        "        \n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <Data_Analysis.get_team_plan_opps_won()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "            return pd.DataFrame() \n",
        "            \n",
        "    def get_sse_rev_impact(self, is_acct_record_type=['UFB', 'UFG'], is_opp_product_type=['Team Plan']):\n",
        "        \n",
        "        '''\n",
        "            Returns:  S&SE Revenue Impact based on SalesForce Cases opened. Excludes Team Plan,  NV/Reseller Opportunities.  \n",
        "            \n",
        "            Parameters:\n",
        "                None\n",
        "            \n",
        "            Returns:\n",
        "               aggs_ (Pandas GroupBy DataFrame): aggs_ Pandas GroupBy outputs into one DataFrame'. \n",
        "        '''\n",
        "        # -*- SSE Cases with Opps Won and Lost-*-\n",
        "        try:\n",
        "            \n",
        "            # -*- Get Opps WON w/ S&SE Engagement Cases -*-  \n",
        "            df = self.acct_opps.loc[idx[:, :, :, :, self.is_acct_Region_New__c, :, :, self.is_opp_FiscalYears, self.is_opp_FiscalQuarters], :].copy()\n",
        "            df = df.loc[(df.case_Id.notnull())].copy()\n",
        "            \n",
        "            # If Grouping by ['opp_FiscalYear', 'opp_FiscalQuarter'], then calcuate the total number of S&SE Cases grouped by ['case_CreatedYear', 'case_CreatedQuarter']\n",
        "            check = all(item in self.groupby_levels for item in ['opp_FiscalYear', 'opp_FiscalQuarter'])\n",
        "            if check:\n",
        "              groupby_levels=['case_CreatedYear', 'case_CreatedQuarter']\n",
        "            else:\n",
        "              groupby_levels=self.groupby_levels\n",
        "\n",
        "            number_sse_cases = (df.loc[(df.case_Id.notnull())].groupby(level=groupby_levels)\n",
        "                                  .agg(number_sse_cases=('case_Id','count')) # Total number of S&SE Cases\n",
        "                               )\n",
        "            del df\n",
        "            \n",
        "            # -*- Get Opps WON and New ARR and Renewal ARR w/ S&SE Engagement Cases; exclude Team Plan Opps -*-\n",
        "            df = self.acct_opps.loc[idx[self.is_opp_FiscalYears, self.is_opp_FiscalQuarters, ['Closed Won'], self.is_opp_Types, self.is_acct_Region_New__c, :, :, :, :], :].copy()\n",
        "            df = (df.loc[((df.case_Id.notnull())\n",
        "                         & df.acct_Account_Record_Type_Name__c.isin(['UFB', 'UFG'])) \n",
        "                         & (~df.opp_Product_Type__c.isin(['Team Plan'])) \n",
        "                         & (~df.opp_Opportunity_Owner_Role__c.isin(['UFB Sales Team Plan - Mgr']))].copy()\n",
        "                 )\n",
        "            \n",
        "            # Better performance for getting/setting via sorted index; remove dups records by Opp_Id, and ARR fields\n",
        "            df.sort_index(inplace=True)\n",
        "            df.drop_duplicates(subset=['opp_Id', 'opp_NewARR__c_Converted__USD', 'opp_RenewalARR__c_Converted__USD'], inplace=True)\n",
        "\n",
        "            # Groupby 'Closed Won' Opps DataFrame \n",
        "            group1 = df.groupby(level=self.groupby_levels).agg(agg1=('opp_Id', 'nunique'), \n",
        "                                                               agg2=('opp_NewARR__c_Converted__USD', np.sum),\n",
        "                                                               agg3=('opp_RenewalARR__c_Converted__USD', np.sum)\n",
        "                                                               )\n",
        "            number_sse_cases.index.names = group1.index.names\n",
        "            group1 = pd.merge(group1, number_sse_cases, left_index=True, right_index=True, copy=True)\n",
        "            del df\n",
        "\n",
        "            # -*- Get Opps LOST and New ARR and Renewal ARR w/ S&SE Engagement Cases -*-\n",
        "            df = self.acct_opps.loc[idx[self.is_opp_FiscalYears, self.is_opp_FiscalQuarters, ['Closed Lost'], self.is_opp_Types, self.is_acct_Region_New__c, :, :, :, :], :].copy()\n",
        "            df = (df.loc[(df.acct_Account_Record_Type_Name__c.isin(is_acct_record_type)) \n",
        "                         & (df.case_Id.notnull())\n",
        "                         & (~df.opp_Product_Type__c.isin(['Team Plan'])) \n",
        "                         & (~df.opp_Opportunity_Owner_Role__c.isin(['UFB Sales Team Plan - Mgr']))].copy())\n",
        "\n",
        "            # Better performance for getting/setting via sorted index; remove dups records by Opp_Id, and ARR fields\n",
        "            df.sort_index(inplace=True)\n",
        "            df.drop_duplicates(subset=['opp_Id', 'opp_NewARR__c_Converted__USD', 'opp_RenewalARR__c_Converted__USD'], inplace=True)\n",
        "\n",
        "            # Groupby 'Closed Lost' Opps DataFrame \n",
        "            group2 = df.groupby(level=self.groupby_levels).agg(agg1_=('opp_Id', 'nunique'), \n",
        "                                                               agg2_=('opp_NewARR__c_Converted__USD', np.sum), \n",
        "                                                               agg3_=('opp_RenewalARR__c_Converted__USD', np.sum)\n",
        "                                                               )\n",
        "            # Merge Grouped DataFrames\n",
        "            group1.sort_index(axis=0, ascending=True, inplace=True)\n",
        "            group2.sort_index(axis=0, ascending=True, inplace=True)\n",
        "            aggs_ = pd.merge(group2[['agg1_', 'agg2_', 'agg3_']], group1, left_index=True, right_index=True, copy=True)\n",
        " \n",
        "            # Rename Columns\n",
        "            aggs__rename_mapper = {'agg1': 'no_sse_opps_won', \n",
        "                                   'agg2': 'SSE_IMPACT_NARR_C',\n",
        "                                   'agg3': 'SSE_IMPACT_RARR_C',\n",
        "                                   'agg1_': 'no_sse_opps_lost',\n",
        "                                   'agg2_': 'SSE_NARR_C_LOST',\n",
        "                                   'agg3_': 'SSE_RARR_C_LOST'\n",
        "                                }\n",
        "            aggs_.rename(columns=aggs__rename_mapper, inplace=True)\n",
        "\n",
        "            aggs__label_formatter = {}\n",
        "            for k, v in aggs__rename_mapper.items():\n",
        "                if 'ARR' in v:\n",
        "                    aggs__label_formatter[v] = '${:,.0f}'\n",
        "                else:\n",
        "                     aggs__label_formatter[v] = '{:,.0f}'\n",
        "\n",
        "            # Style output DataFrame\n",
        "            output_labels = ['no_sse_opps_won', 'number_sse_cases', 'SSE_IMPACT_NARR_C', 'SSE_IMPACT_RARR_C', 'no_sse_opps_lost', 'SSE_NARR_C_LOST', 'SSE_RARR_C_LOST']\n",
        "            gradient_subset_labels = ['SSE_IMPACT_NARR_C', 'SSE_IMPACT_RARR_C', 'SSE_NARR_C_LOST', 'SSE_RARR_C_LOST'] \n",
        "            aggs__styled = (aggs_[output_labels].style.format(aggs__label_formatter).background_gradient(cmap=self.cm, subset=gradient_subset_labels))\n",
        "\n",
        "            self.sse_rev_impact_grp = aggs__styled\n",
        "            del df, aggs__styled\n",
        "            return aggs_\n",
        "        \n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <Data_Analysis.get_sse_rev_impact()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "            return pd.DataFrame() \n",
        "            \n",
        "    def get_sse_rev_impact_analysis(self):\n",
        "        \n",
        "        '''\n",
        "        Returns S&SE Team's Revenue Impact on UB Opportunitied; excludes Team Plan, NV/Reseller Opportunities\n",
        "        \n",
        "        Parameters:\n",
        "            None\n",
        "        \n",
        "        Returns\n",
        "            aggs_2 (Pandas GroupBy DataFrame): aggs_ GroupBy DataFrame of S&SE Revenue Impact\n",
        "            aggs_2__styled (Pandas GroupBy DataFrame Styled): Styled aggs_2 output\n",
        "        '''\n",
        "        \n",
        "        # -*-SSE Revenue Impact on Opps Won-*-\n",
        "        try:    \n",
        "            # First, Get Grouped Dataframes for Opps Won/Lost, SSE ARR Impact, and Team Plan ARR\n",
        "            grp1 = self.get_opps_closed_won_lost()\n",
        "            grp2 = self.get_sse_rev_impact()\n",
        "            \n",
        "            # Now, merge the above Grouped Dataframes\n",
        "            aggs_1 = pd.merge(grp1, grp2, left_index=True, right_index=True, copy=True)\n",
        "            # aggs_2.drop(columns=['opp_FiscalYear_x', 'opp_FiscalQuarter_x', 'opp_FiscalYear_y', 'opp_FiscalQuarter_y'], inplace=True)\n",
        "            del grp1, grp2\n",
        "\n",
        "            # Rename labels\n",
        "            aggs_1_rename_mapper = {\n",
        "                                'current_column_label': 'new_column_label'\n",
        "            }\n",
        "\n",
        "            # Calculate SSE Impact Ratio Data\n",
        "            aggs_1['SSE_IMPACT_NARR_C_RATIO'] = aggs_1.SSE_IMPACT_NARR_C/(aggs_1.NARR_C)\n",
        "            aggs_1['SSE_IMPACT_RARR_C_RATIO'] = aggs_1.SSE_IMPACT_RARR_C/(aggs_1.RARR_C)\n",
        "            aggs_1['SSE_IMPACT_NARR_C_LOST_RATIO'] = aggs_1.SSE_NARR_C_LOST/(aggs_1.NARR_C_Lost)  \n",
        "            aggs_1['SSE_IMPACT_RARR_C_LOST_RATIO'] = aggs_1.SSE_RARR_C_LOST/(aggs_1.RARR_C_Lost)  \n",
        "            \n",
        "            # Format label outputs\n",
        "            aggs_1_format_dict = {\n",
        "                                    'NARR_C': '${:,.0f}',\n",
        "                                    'RARR_C': '${:,.0f}',\n",
        "                                    'NARR_C_Lost': '${:,.0f}',\n",
        "                                    'RARR_C_Lost': '${:,.0f}',\n",
        "                                    'SSE_IMPACT_NARR_C': '${:,.0f}',\n",
        "                                    'SSE_IMPACT_RARR_C': '${:,.0f}',\n",
        "                                    'SSE_NARR_C_LOST': '${:,.0f}',\n",
        "                                    'SSE_RARR_C_LOST': '${:,.0f}',\n",
        "                                    'SSE_IMPACT_NARR_C_RATIO': '{:.2%}',\n",
        "                                    'SSE_IMPACT_RARR_C_RATIO': '{:.2%}',\n",
        "                                    'SSE_IMPACT_NARR_C_LOST_RATIO': '{:.2%}',\n",
        "                                    'SSE_IMPACT_RARR_C_LOST_RATIO': '{:.2%}',\n",
        "                            }\n",
        "    \n",
        "            # Stylize DataFrame Output\n",
        "            output_labels=[ \n",
        "                            'no_of_opps_won', 'no_sse_opps_won', 'no_of_opps_lost', 'no_sse_opps_lost', 'number_sse_cases', \n",
        "                            'NARR_C', 'SSE_IMPACT_NARR_C', 'SSE_IMPACT_NARR_C_RATIO', 'RARR_C', 'SSE_IMPACT_RARR_C', 'SSE_IMPACT_RARR_C_RATIO',\n",
        "                            'NARR_C_Lost', 'SSE_NARR_C_LOST', 'SSE_IMPACT_NARR_C_LOST_RATIO', 'RARR_C_Lost', 'SSE_RARR_C_LOST', 'SSE_IMPACT_RARR_C_LOST_RATIO'\n",
        "                          ]\n",
        "            background_gradient_subset=['NARR_C', 'SSE_IMPACT_NARR_C', 'SSE_IMPACT_NARR_C_RATIO', 'RARR_C', 'SSE_IMPACT_RARR_C', 'SSE_IMPACT_RARR_C_RATIO', \n",
        "                                        'NARR_C_Lost', 'SSE_NARR_C_LOST', 'SSE_IMPACT_NARR_C_LOST_RATIO', 'RARR_C_Lost', 'SSE_RARR_C_LOST', 'SSE_IMPACT_RARR_C_LOST_RATIO']\n",
        "            \n",
        "            # aggs_2.reset_index(drop=False, inplace=True)\n",
        "            aggs_1__styled = (aggs_1[output_labels].style.format(aggs_1_format_dict)\n",
        "                                                     .background_gradient(cmap=self.cm, subset=background_gradient_subset)\n",
        "                                                     .set_caption('S&SE ARR Performace: {0}'.format(aggs_1.index.unique()))\n",
        "                                                     )\n",
        "\n",
        "            return aggs_1, aggs_1__styled\n",
        "        \n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <Data_Analysis.get_sse_rev_impact_analysis()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "            return pd.DataFrame()       \n",
        "    \n",
        "    def get_corpU_opp_stats(self,):\n",
        "        '''\n",
        "        Returns Grouped Pandas DataFrames for Learning Academy Opportunities in SalesForce in Current Year\n",
        "        \n",
        "        Parameters:\n",
        "            group_levels (list[int]): Multi-index levels to group by. The DataFrame must be a Multi-Index aligned DataFrame\n",
        "        \n",
        "        Returns:\n",
        "            df_grp (Pandas DataFrame): Grouped Pandas DataFrame of Learning Academy Opportunities\n",
        "            df_pvt (Pandas DataFrame): PivotTable Pandas DataFrame of Learning Academy Opportunities\n",
        "        '''\n",
        "        # -*-CorpU Opps-*-\n",
        "        try:\n",
        "            \n",
        "            # Filter the DataFrame for CorpU Opportunities\n",
        "            df = self.acct_opps.loc[idx[[pd.Timestamp(datetime.now()).year], :, self.is_opp_Stages_Open, :, self.is_acct_Region_New__c, :, :, :, :], :].copy()\n",
        "            df = df.loc[((df.opp_CorpU__c) | (df.opp_Opportunity_Name_Append__c.str.contains('CorpU')))].copy()\n",
        "            \n",
        "            # Faster get/setting via sorted index; drop Opp dups\n",
        "            df.sort_index(inplace=True)\n",
        "            df.drop_duplicates(subset=['opp_Id'], inplace=True)\n",
        "\n",
        "            # -*- DATA FRAME 1: GroupBy -*-\n",
        "            df_grp = df.groupby(level=self.groupby_levels).agg(agg1=('opp_Id', np.count_nonzero), \n",
        "                                                               agg2=('opp_NewARR__c_Converted__USD', np.sum))\n",
        "\n",
        "            # Flatten Groupby DataFrame & rename columns\n",
        "            df_grp.reset_index(inplace=True)\n",
        "            df_rename_mapper = {\n",
        "                                'agg1': 'NoOpps', \n",
        "                                'agg2': 'NARR'\n",
        "            }      \n",
        "            df_grp.rename(columns=df_rename_mapper, inplace=True)\n",
        "\n",
        "            # Remove missing values (nan)\n",
        "            fill_na={}\n",
        "            for column in df_grp.columns:\n",
        "                if (df_grp[column].dtype) in (np.int64, np.float64):\n",
        "                    fill_na[column]=0\n",
        "            df_grp.fillna(fill_na, inplace=True)\n",
        "\n",
        "            # Define Format Labels\n",
        "            df_label_formatter = {}  \n",
        "            df_columns = ['{}'.format(field) for field in list(df_grp.columns)]\n",
        "            for column in list(df_grp.columns):\n",
        "                if 'ARR' in column:\n",
        "                    df_label_formatter[column] = '${:,.0f}'\n",
        "                else:\n",
        "                    df_label_formatter[column] = '{}'\n",
        "\n",
        "            # Style Output DataFrame\n",
        "            df_grp_styled = (df_grp.style\n",
        "                             .format(df_label_formatter)\n",
        "                             .background_gradient(cmap=self.cm, subset=['NoOpps', 'NARR'])\n",
        "                             .set_caption('CorpU NARR Opps [{}] ${:,.0f}\\n'.format([pd.Timestamp(datetime.now()).year], df_grp.NARR.values.sum())\n",
        "                                         )\n",
        "                            )\n",
        "\n",
        "            # -*- DATA FRAME 2: PivotTable -*-\n",
        "            index=['opp_FiscalQuarter']\n",
        "            columns=['opp_ForecastCategoryName']\n",
        "            df.reset_index(inplace=True)\n",
        "            df_columns_subset=['opp_FiscalQuarter', 'opp_Id', 'opp_NewARR__c_Converted__USD', 'opp_ForecastCategoryName']\n",
        "            df_pvt = pd.pivot_table(data=df[df_columns_subset], \n",
        "                                    values=None, \n",
        "                                    index=index, \n",
        "                                    columns=columns, \n",
        "                                    aggfunc={'opp_NewARR__c_Converted__USD': np.sum,\n",
        "                                             'opp_Id': np.count_nonzero},\n",
        "                                    margins=False,\n",
        "                                    margins_name='Total',\n",
        "                                    fill_value=0\n",
        "                                   )\n",
        "\n",
        "            # Remove missing values (nan)\n",
        "            fill_na={}\n",
        "            for column in df_pvt.columns:\n",
        "                if (df_pvt[column].dtype) in (np.int64, np.float64):\n",
        "                    fill_na[column]=0\n",
        "            df_pvt.fillna(fill_na, inplace=True)\n",
        "\n",
        "            # Output Formatting\n",
        "            df_label_formatter = {}\n",
        "            if isinstance(df_pvt.columns, pd.MultiIndex):\n",
        "                for k, v, in df_pvt.columns:\n",
        "                    if (('ARR' in k) | ('ARR' in v)):\n",
        "                        df_label_formatter[(k,v)] = '${:,.0f}'\n",
        "                    else:\n",
        "                        df_label_formatter[(k,v)] = '{}'\n",
        "            else: \n",
        "                for column in df_pvt.columns:\n",
        "                    if 'ARR' in column:\n",
        "                        df_label_formatter[column] = '${:,.0f}'\n",
        "                    else:\n",
        "                        df_label_formatter[column] = '{}'\n",
        "            \n",
        "            df_pvt_styled = (df_pvt.style\n",
        "                             .format(df_label_formatter)\n",
        "                             .background_gradient(cmap=self.cm)\n",
        "                             .set_caption('{} CorpU Open Opps NARR Total: ${:,.0f}\\n'.format([pd.Timestamp(datetime.now()).year], df_grp.NARR.values.sum()))\n",
        "                            )\n",
        "\n",
        "            del df\n",
        "            return (df_grp, df_grp_styled, df_pvt, df_pvt_styled)\n",
        "        \n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <Data_Analysis.get_corpU_opp_stats()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "            return pd.DataFrame()         \n",
        "\n",
        "    def get_pivot_table_analysis(self, data=pd.DataFrame(), index=list(), columns=list(), values=list(), drop_opp_dups=True, include_total=True, sort_asc=True):\n",
        "\n",
        "      '''\n",
        "        Returns multiple Pandas PivoTables for specified Data and Group Parameters\n",
        "        \n",
        "        Parameters:\n",
        "            Data (Pandas Data Object): to apply PivotTable [Required]\n",
        "            index (list): list to index on in Pandas PivotTable [Required]\n",
        "            columns (list): list for Column headers in Pandas PivoTable [Required]\n",
        "            values (list): list for Column Names in DataFrame to create aggregations [Required]\n",
        "            drop_opp_dups (boolean): list for Column headers in Pandas PivoTable [Required]\n",
        "        \n",
        "        Returns\n",
        "            aggs_2 (Pandas GroupBy DataFrame): aggs_ GroupBy DataFrame of S&SE Revenue Impact\n",
        "            aggs_2__styled (Pandas GroupBy DataFrame Styled): Styled aggs_2 output\n",
        "      '''   \n",
        "\n",
        "      try:\n",
        "            pvt_pctg1=pd.DataFrame() \n",
        "            pvt_pctg2=pd.DataFrame()\n",
        "            # Better performance for getting/setting via sorted index; drop Opp_Id\n",
        "            data.sort_index(inplace=True)\n",
        "            if drop_opp_dups:\n",
        "              data.drop_duplicates(subset=['opp_Id'], inplace=True)\n",
        "\n",
        "            # Build Agg for PivoTable from Values passed\n",
        "            if not values:\n",
        "              return None\n",
        "            if values:\n",
        "              agg_func={}\n",
        "              for value in values:\n",
        "                if 'ARR' in value:\n",
        "                  agg_func[value]=np.sum\n",
        "                  # agg_func[value]=np.mean\n",
        "                  # agg_func[value]=np.median\n",
        "                elif '_Id' in value:\n",
        "                  agg_func[value]=np.count_nonzero\n",
        "                else:\n",
        "                  agg_func[value]=np.count_nonzero\n",
        "            print(agg_func)\n",
        "            pvt = (pd.pivot_table(data=data.reset_index(), \n",
        "                                  values=values, \n",
        "                                  index=index, \n",
        "                                  columns=columns,\n",
        "                                  aggfunc=agg_func, \n",
        "                                  margins=include_total,\n",
        "                                  dropna=False,\n",
        "                                  fill_value=0\n",
        "                                  )\n",
        "                  )\n",
        "            pvt.sort_values(by=[pvt.columns[-1]], ascending=False, inplace=True)\n",
        "            \n",
        "            # Calculate Pctg Change over Grand Total\n",
        "            if include_total:\n",
        "              pvt_pctg1 = pd.DataFrame()\n",
        "              grd_total = pvt.loc[idx['All'], pvt.columns[::-len(pvt.columns)]].values\n",
        "              arr = ((pvt.values/grd_total))\n",
        "              pvt_pctg1 = pd.DataFrame(arr, columns=pvt.columns, index=pvt.index)\n",
        "            \n",
        "              # Calculate Pctg Change over Row Index Total\n",
        "              pvt_pctg2 = pd.DataFrame()\n",
        "              arr = np.nan_to_num(pvt.values/pvt.head(n=1).values)\n",
        "              pvt_pctg2 = pd.DataFrame(arr, columns=pvt.columns, index=pvt.index)\n",
        "            \n",
        "              # Sort by the Last Column in the DataFrame; if margings=True in Pivot Table, sorts by index Total\n",
        "              if sort_asc:\n",
        "                pvt_pctg1.sort_values(by=[pvt.columns[-1]], ascending=False, inplace=True)\n",
        "                pvt_pctg2.sort_values(by=[pvt.columns[-1]], ascending=False, inplace=True)\n",
        "            \n",
        "            # pvt.fillna(value=0, inplace=True)\n",
        " \n",
        "            return (pvt, pvt_pctg1, pvt_pctg2)\n",
        "\n",
        "      except Exception as error:\n",
        "          sys.stdout.write('\\n {0} *** Error in <Data_Analysis.get_pivot_table_analysis()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "          return pd.DataFrame() \n",
        "    \n",
        "    def style_data_frame(self, data=pd.DataFrame(), pct_format=False):\n",
        "        '''\n",
        "        Returns Styled Pandas DataFrame Object \n",
        "        \n",
        "            Parameters:\n",
        "                data: Pandas DataFrame Object\n",
        "            \n",
        "            Returns:\n",
        "                data_styled (Pandas DataFrame Styled Object): Pandas DataFrame GroupBy Styled Object\n",
        "        ''' \n",
        "        try:\n",
        "\n",
        "                # Replace missing values (nan) for int or float columns\n",
        "                fill_na={}\n",
        "                for column in data.columns:\n",
        "                    if (data[column].dtype) in (np.int64, np.float64):\n",
        "                        fill_na[column]=0\n",
        "                data.fillna(fill_na, inplace=True)\n",
        "                \n",
        "                # Style DataFrame as Currenty or Percentage\n",
        "                data_label_formatter={}\n",
        "                if isinstance(data.columns, pd.MultiIndex): \n",
        "                  sub = 'ARR'\n",
        "                  for columns in list(data.columns):\n",
        "                    for col in list(columns):\n",
        "                      if (any(s for s in ['{0}'.format(col)] if sub.lower() in s.lower())):\n",
        "                        data_label_formatter[columns] = '${:,.0f}'\n",
        "                if pct_format:\n",
        "                   for columns in list(data.columns):\n",
        "                     data_label_formatter[columns] = '{:.2%}'\n",
        "                \n",
        "                # Tool Tip highlighting\n",
        "                tt = pd.DataFrame([['-']*len(data.columns)], \n",
        "                                  index=['All'], \n",
        "                                  columns=data.columns\n",
        "                                ) \n",
        "                last_column = data.columns[::-len(data.columns)]\n",
        "                cell_border = pd.DataFrame([['border-blue ']*len(data.columns[::-len(data.columns)])], index=['All'], columns=data.columns[::-len(data.columns)])\n",
        "                data_styled = (data.style.format(formatter=data_label_formatter)\n",
        "                .background_gradient(cmap=sns.color_palette('rocket_r', as_cmap=True), subset=list(data_label_formatter.keys()))\n",
        "                .set_tooltips(tt, props='visibility: hidden; position: absolute; z-index: 1; border: 2px solid #242CB1;'\n",
        "                                        'background-color: white; color: #000066; font-size: 1em;'\n",
        "                                        'transform: translate(0px, -24px); padding: 0.6em; border-radius: 0.5em;')\n",
        "                              .set_table_styles(\n",
        "                                  [\n",
        "                                      {\n",
        "                                        'selector': '.border-red', 'props': 'border: 2px dashed red;'\n",
        "                                        }, \n",
        "                                      {'selector': '.border-green', 'props': 'border: 2px dashed green;'\n",
        "                                        }, \n",
        "                                      {'selector': '.border-blue', 'props': 'border: 2px dashed blue;'\n",
        "                                        }\n",
        "                                    ], \n",
        "                                    overwrite=False)\n",
        "                              .set_td_classes(cell_border)\n",
        "                )\n",
        "            \n",
        "                return data_styled\n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <Data_Analysis.style_data_frame()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "            return pd.DataFrame()  \n",
        "\n",
        "    def chart_missing_data(self, data=pd.DataFrame()):\n",
        "        \n",
        "        '''\n",
        "        Seaboarn Barchart of Missing Data elements in Pandas DataFrame Object\n",
        "            \n",
        "            Paramters:\n",
        "                data (Pandas DataFrame object)\n",
        "            \n",
        "            Returns:\n",
        "                Seaboarn Barchart object\n",
        "        '''       \n",
        "        \n",
        "        try:\n",
        "            missing_data = data.isna().sum().div(data.shape[0]).mul(100).to_frame().sort_values(by=0, ascending=True)\n",
        "            fig, ax = plt.subplots(figsize=(30, 10))\n",
        "            sns.barplot(x=missing_data[missing_data[0]>0].index, y=missing_data[missing_data[0]>0][0])\n",
        "            plt.ylabel('Pctg Of Missing Values in Data', fontsize=15, fontweight='bold')\n",
        "            plt.xlabel('Features', fontsize=15, fontweight='bold')\n",
        "            plt.xticks(rotation=90)\n",
        "\n",
        "            return plt.show()\n",
        "       \n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <Data_Analysis.chart_missing_data()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "            return pd.DataFrame() \n",
        "        \n",
        "    def display(self, data_frames=dict()):\n",
        "        '''\n",
        "        Returns Modal of Pandas DataFrame Object\n",
        "        \n",
        "        Parameters:\n",
        "            data_frames (Pandas DataFrame Object): Pandas DataFrame to display in Jupyter Modal Window\n",
        "        Returns:\n",
        "            None\n",
        "        '''\n",
        "        try:\n",
        "            # Display Output\n",
        "            for key in data_frames.keys():\n",
        "                sc = Sidecar(title=str(key), anchor='split-bottom')\n",
        "                with sc:\n",
        "                    display(data_frames[key])\n",
        "        except Exception as error:\n",
        "            sys.stdout.write('\\n {0} *** Error in <Data_Analysis.display()> *** {1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))\n",
        "            return 0\n",
        "                \n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Seed Data and Filters\n",
        "        da = Data_Analysis(acct_opps=acct_opps.copy(), is_opp_FiscalYears=[2020, 2021, 2022, 2023], is_opp_FiscalQuarters=[1, 2, 3, 4], is_acct_Region_New__c=All, groupby_levels=['opp_FiscalYear', 'opp_FiscalQuarter'])\n",
        "        sys.stdout.write('\\n{0} . . . Data Analysis'.format(datetime.now()))\n",
        "    except Exception as error:\n",
        "        sys.stdout.write('\\n{0} *** Error in <Data_Analysis__main__> ***{1}\\n{2}'.format('\\x1b[0;31;40m', '\\x1b[0m', traceback.format_exc()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "R6q4O3dBZnlE",
        "outputId": "31b86203-ae84-4719-8479-4b46a7216043"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "          <style>\n",
              "            pre {\n",
              "                white-space: pre-wrap;\n",
              "            }\n",
              "          </style>\n",
              "          "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2023-04-28 16:13:17.803815 . . . Data Analysis"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #### Global S&SE Revenue Performance Metrics { run: \"auto\" }\n",
        "\n",
        "# Form Filters\n",
        "is_opp_FiscalYears_list = All #@param [\"All\", \"[2021]\", \"[2022]\", \"[2023]\", \"[2021, 2022, 2023]\"] {type:\"raw\", allow-input: true}\n",
        "is_opp_FiscalQuarters_list = All #@param [\"All\", \"[1]\", \"[2]\", \"[3]\", \"[4]\"] {type:\"raw\", allow-input: true}\n",
        "is_acct_Region_New__c_list = All #@param [\"All\", \"\\\"APAC\\\"\", \"\\\"AMER\\\"\", \"\\\"EMEA\\\"\", \"\\\"LATAM\\\"\", \"\\\"AMER LATAM\\\"\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "# Convert values to List if not a Slice (i.e., not 'All' is selected)\n",
        "if not isinstance(is_opp_FiscalYears_list, slice):\n",
        "  is_opp_FiscalYears_list=list(is_opp_FiscalYears_list)\n",
        "if not isinstance(is_opp_FiscalQuarters_list, slice):\n",
        "  is_opp_FiscalQuarters_list=list(is_opp_FiscalQuarters_list)\n",
        "if not isinstance(is_acct_Region_New__c_list, slice):\n",
        "  is_acct_Region_New__c_list=is_acct_Region_New__c_list.split()\n",
        "\n",
        "print(is_opp_FiscalYears_list,is_opp_FiscalQuarters_list,is_acct_Region_New__c_list)\n",
        "button = widgets.Button(description=\"Run Analysis!\")\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_button_clicked(b):\n",
        "  # Display the message within the output widget.\n",
        "  with output:\n",
        "    da = Data_Analysis(acct_opps=acct_opps.copy(),\n",
        "                       is_opp_FiscalYears=is_opp_FiscalYears_list, \n",
        "                       is_opp_FiscalQuarters=is_opp_FiscalQuarters_list, \n",
        "                       is_acct_Region_New__c=is_acct_Region_New__c_list, \n",
        "                       groupby_levels=['opp_FiscalYear', 'opp_FiscalQuarter'])\n",
        "    da.display(data_frames={'S&SE ARR Impact': da._data_['sse_rev_impact_analysis_styled']})\n",
        "button.on_click(on_button_clicked)\n",
        "display(button, output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "cd8c4cabeb7643109da7a87f38face91",
            "cee0cc33c26a407dbdeb4e0d5ea18b5e",
            "b6694b74d5cb4c938af920eef303a400",
            "026d915afb054d909c95299fd0ad1ed8",
            "a1ed0f7c5c5a40ba8edd0c9738487dc3"
          ]
        },
        "id": "bHdekteMbeVE",
        "outputId": "710f97fd-96b0-49d1-e56d-3eda158cba79"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "          <style>\n",
              "            pre {\n",
              "                white-space: pre-wrap;\n",
              "            }\n",
              "          </style>\n",
              "          "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "          <style>\n",
              "            pre {\n",
              "                white-space: pre-wrap;\n",
              "            }\n",
              "          </style>\n",
              "          "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "slice(None, None, None) slice(None, None, None) slice(None, None, None)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Run Analysis!', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd8c4cabeb7643109da7a87f38face91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "026d915afb054d909c95299fd0ad1ed8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #### Global S&SE Case Engagement Metrics { run: \"auto\" }\n",
        "# Form Filters\n",
        "is_opp_FiscalYears_list = All #@param [\"All\", \"[2021]\", \"[2022]\", \"[2023]\", \"[2021, 2022, 2023]\"] {type:\"raw\", allow-input: true}\n",
        "is_opp_FiscalQuarters_list = All #@param [\"All\", \"[1]\", \"[2]\", \"[3]\", \"[4]\"] {type:\"raw\", allow-input: true}\n",
        "is_acct_Region_New__c_list = All #@param [\"All\", \"\\\"APAC\\\"\", \"\\\"AMER\\\"\", \"\\\"EMEA\\\"\", \"\\\"LATAM\\\"\", \"\\\"AMER, LATAM\\\"\"] {type:\"raw\", allow-input: true}\n",
        "is_opp_StageNames_list = All #@param [\"All\", \"\\\"Closed Won\\\"\", \"\\\"Closed Lost\\\"\"] {type:\"raw\", allow-input: true}\n",
        "is_opp_Types_list = \"ARR Subset\" #@param [\"All\", \"ARR Subset\"] {allow-input: true}\n",
        "is_case_CreatedYears_list = [2023] #@param [\"All\", \"[2021]\", \"[2022]\", \"[2023]\", \"[2021, 2022, 2023]\"] {type:\"raw\", allow-input: true}\n",
        "is_case_CreatedQuarters_list = [1] #@param [\"All\", \"[1]\", \"[2]\", \"[3]\", \"[4]\"] {type:\"raw\", allow-input: true}\n",
        "\n",
        "# Convert values to List if not a Slice (i.e., not 'All' is selected)\n",
        "if not isinstance(is_opp_FiscalYears_list, slice):\n",
        "  is_opp_FiscalYears_list=list(is_opp_FiscalYears_list)\n",
        "if not isinstance(is_opp_FiscalQuarters_list, slice):\n",
        "  is_opp_FiscalQuarters_list=list(is_opp_FiscalQuarters_list)\n",
        "if not isinstance(is_acct_Region_New__c_list, slice):\n",
        "  is_acct_Region_New__c_list=is_acct_Region_New__c_list.split()\n",
        "if not isinstance(is_opp_StageNames_list, slice):\n",
        "  is_opp_StageNames_list=is_opp_StageNames_list.split('  ', maxsplit=1)\n",
        "if not isinstance(is_case_CreatedYears_list, slice):\n",
        "  is_case_CreatedYears_list=list(is_case_CreatedYears_list)\n",
        "if not isinstance(is_case_CreatedYears_list, slice):\n",
        "  is_case_CreatedQuarters_list=list(is_case_CreatedQuarters_list)\n",
        "if is_opp_Types_list == 'All':\n",
        "  is_opp_Types_list=All\n",
        "if is_opp_Types_list== 'ARR Subset':\n",
        "  is_opp_Types_list=da.is_opp_Types\n",
        "\n",
        "print('[{0}, {1}, {2}, {3}, {4}, {5}, {6}]'.format(is_opp_FiscalYears_list, is_opp_FiscalQuarters_list, \n",
        "                                                   is_acct_Region_New__c_list, is_opp_StageNames_list, \n",
        "                                                   is_opp_Types_list, is_case_CreatedYears_list, \n",
        "                                                   is_case_CreatedQuarters_list))\n",
        "\n",
        "data=acct_opps.loc[idx[is_opp_FiscalYears_list, \n",
        "                       is_opp_FiscalQuarters_list, \n",
        "                       is_opp_StageNames_list, \n",
        "                       is_opp_Types_list, \n",
        "                       is_acct_Region_New__c_list, \n",
        "                       :, \n",
        "                       :, \n",
        "                       is_case_CreatedYears_list, \n",
        "                       is_case_CreatedQuarters_list], \n",
        "                   :].copy()\n",
        "data=(data.loc[(data.case_Id.notnull())\n",
        "& (data.acct_Account_Record_Type_Name__c.isin(['UFB', 'UFG']) )\n",
        "& (~data.opp_Product_Type__c.isin(['Team Plan'])) \n",
        "& (~data.opp_Opportunity_Owner_Role__c.isin(['UFB Sales Team Plan - Mgr']))\n",
        "& (data.loc[data.opp_UFB_Sales_Owner_Segment__c.isin(['ENT'])])\n",
        "& (data.case_SSEOwnerName.isin(list(sf.sse_assignments['AMER'][0].keys())))\n",
        "])\n",
        "if data.empty:\n",
        "  print('No Data')\n",
        "  exit()\n",
        "\n",
        "button = widgets.Button(description=\"Run Analysis!\")\n",
        "output = widgets.Output()\n",
        "\n",
        "# {'index_': ['opp_FiscalYear', \n",
        "#             'opp_FiscalQuarter',\n",
        "#             'opp_StageName',\n",
        "#             'opp_Type',\n",
        "#             'acct_Region_New__c',\n",
        "#             'acct_Region__c',\n",
        "#             'acct_Business_Segment__c',\n",
        "#             'case_CreatedYear',\n",
        "#             'case_CreatedQuarter'\n",
        "#             ],\n",
        "#  'acct_fields': ['acct_Id', \n",
        "#                  'acct_Employee_Segment__c',\n",
        "#                  'acct_Account_Owner_Text__c'\n",
        "#                   ],\n",
        "#  'opp_fields': ['opp_Id', \n",
        "#                 'opp_FiscalYear',\n",
        "#                 'opp_StageName',\n",
        "#                 'opp_ForecastCategory',\n",
        "#                 'opp_Closed_Lost_Details__c',\n",
        "#                 'opp_Lost_Reason__c',\n",
        "#                 'opp_Closed_Lost_Reason__c',\n",
        "#                 'opp_Closed_Lost_Details__c',\n",
        "#                 'opphist_StageName',\n",
        "#                 'opp_SE_Assigned',\n",
        "#                 'opp_Contract_Length_CPQ__c_Years',\n",
        "#                 'opphist_StageName'\n",
        "#                 ],\n",
        "#  'opp_arr_fields': ['opp_NewARR__c_Converted__USD', \n",
        "#                     'opp_RenewalARR__c_Converted__USD',\n",
        "#                     'case_SSE_Percentage_Impact',\n",
        "#                     'opp_NewARR__c_Converted__USD_SSE_IMPACT',\n",
        "#                     'opp_RenewalARR__c_Converted__USD_SSE_IMPACT'\n",
        "#                     ],\n",
        "#  'case_fields': ['case_Id',\n",
        "#                  'case_SSEOwnerName',\n",
        "#                  'case_Creator_Team',\n",
        "#                  'case_Type',\n",
        "#                  'case_Sub_Type__c'\n",
        "#                  ]\n",
        "#  }\n",
        "\n",
        "def on_button_clicked(b):\n",
        "  # Display the message within the output widget.\n",
        "  with output:\n",
        "    index = ['case_SSEOwnerName']\n",
        "    columns = ['case_CreatedWeek']\n",
        "    values = ['case_Id']\n",
        "    (pvt, pvt_pctg1, pvt_pctg2) = da.get_pivot_table_analysis(data=data, \n",
        "                                                              index=index, \n",
        "                                                              columns=columns, \n",
        "                                                              values=values, \n",
        "                                                              drop_opp_dups=False, \n",
        "                                                              include_total=True, \n",
        "                                                              sort_asc=False)\n",
        "    (da.display(data_frames={'S&SE ARR Impact': \n",
        "                             (da.style_data_frame(data=pvt\n",
        "                                                  # .loc[:, ('opp_NewARR__c_Converted__USD', 'AMER')]\n",
        "                                                  # .loc[pvt[('opp_NewARR__c_Converted__USD', 'All')]>0]\n",
        "                                                  # .sort_index()\n",
        "                                                  )\n",
        "                             )\n",
        "                             }\n",
        "                )\n",
        "    )\n",
        "button.on_click(on_button_clicked)\n",
        "display(button, output)"
      ],
      "metadata": {
        "id": "m6GJMxUyR7B2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07bc368f-3f96-4b8f-c244-fe6519bfdadc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "          <style>\n",
              "            pre {\n",
              "                white-space: pre-wrap;\n",
              "            }\n",
              "          </style>\n",
              "          "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, None, None), ['New Business', 'Upsell', 'Upgrade', 'Expansion', 'Renewal with Upsell', 'Renewal'], [2023], [1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pvt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "x-2ODH4EDDys",
        "outputId": "5771eed3-7d15-4894-fb75-53d2b40139d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "          <style>\n",
              "            pre {\n",
              "                white-space: pre-wrap;\n",
              "            }\n",
              "          </style>\n",
              "          "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "          <style>\n",
              "            pre {\n",
              "                white-space: pre-wrap;\n",
              "            }\n",
              "          </style>\n",
              "          "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                 opp_NewARR__c_Converted__USD                 \\\n",
              "acct_Employee_Segment__c                    1: 1â€“199 Emerging 2: 200â€“999 SMB   \n",
              "opp_FiscalYear opp_FiscalQuarter                                               \n",
              "All                                             580881.875000     1821366.50   \n",
              "2022           1                                260579.171875      937194.25   \n",
              "2023           1                                320302.718750      884172.25   \n",
              "\n",
              "                                                            \\\n",
              "acct_Employee_Segment__c         3: 1,000â€“5,000 Mid Market   \n",
              "opp_FiscalYear opp_FiscalQuarter                             \n",
              "All                                             1203008.75   \n",
              "2022           1                                 817104.75   \n",
              "2023           1                                 385904.00   \n",
              "\n",
              "                                                                            \n",
              "acct_Employee_Segment__c         4: 5,001+ Enterprise 5: Other         All  \n",
              "opp_FiscalYear opp_FiscalQuarter                                            \n",
              "All                                        8327632.00        0  11932890.0  \n",
              "2022           1                           5280566.50        0   7295445.0  \n",
              "2023           1                           3047065.25        0   4637444.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bcb3de82-2403-44b2-a070-a92f3da3b581\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"6\" halign=\"left\">opp_NewARR__c_Converted__USD</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>acct_Employee_Segment__c</th>\n",
              "      <th>1: 1â€“199 Emerging</th>\n",
              "      <th>2: 200â€“999 SMB</th>\n",
              "      <th>3: 1,000â€“5,000 Mid Market</th>\n",
              "      <th>4: 5,001+ Enterprise</th>\n",
              "      <th>5: Other</th>\n",
              "      <th>All</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>opp_FiscalYear</th>\n",
              "      <th>opp_FiscalQuarter</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>All</th>\n",
              "      <th></th>\n",
              "      <td>580881.875000</td>\n",
              "      <td>1821366.50</td>\n",
              "      <td>1203008.75</td>\n",
              "      <td>8327632.00</td>\n",
              "      <td>0</td>\n",
              "      <td>11932890.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022</th>\n",
              "      <th>1</th>\n",
              "      <td>260579.171875</td>\n",
              "      <td>937194.25</td>\n",
              "      <td>817104.75</td>\n",
              "      <td>5280566.50</td>\n",
              "      <td>0</td>\n",
              "      <td>7295445.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2023</th>\n",
              "      <th>1</th>\n",
              "      <td>320302.718750</td>\n",
              "      <td>884172.25</td>\n",
              "      <td>385904.00</td>\n",
              "      <td>3047065.25</td>\n",
              "      <td>0</td>\n",
              "      <td>4637444.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bcb3de82-2403-44b2-a070-a92f3da3b581')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bcb3de82-2403-44b2-a070-a92f3da3b581 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bcb3de82-2403-44b2-a070-a92f3da3b581');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr1 = pvt.loc[idx[2022, 1], :].values[:-1]/pvt.loc[idx[2022, 1], :].values[-1]\n",
        "arr1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "CJVZ9Y0aDO7g",
        "outputId": "be4f1497-57d8-43c1-92f6-27eccfd22018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "          <style>\n",
              "            pre {\n",
              "                white-space: pre-wrap;\n",
              "            }\n",
              "          </style>\n",
              "          "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "          <style>\n",
              "            pre {\n",
              "                white-space: pre-wrap;\n",
              "            }\n",
              "          </style>\n",
              "          "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.03571806, 0.12846293, 0.11200204, 0.72381692, 0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr2 = pvt.loc[idx[2023, 1], :].values[:-1]/pvt.loc[idx[2023, 1], :].values[-1]\n",
        "arr2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "BQ_4Dy6qDRRx",
        "outputId": "119e7a89-f242-44c3-d495-c1ba9c1f075b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "          <style>\n",
              "            pre {\n",
              "                white-space: pre-wrap;\n",
              "            }\n",
              "          </style>\n",
              "          "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "          <style>\n",
              "            pre {\n",
              "                white-space: pre-wrap;\n",
              "            }\n",
              "          </style>\n",
              "          "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.06906881, 0.19065939, 0.08321481, 0.65705704, 0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr2-arr1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "H1rW-iYpDWJd",
        "outputId": "395d2398-614d-4209-b7df-4d0b3081fe58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "          <style>\n",
              "            pre {\n",
              "                white-space: pre-wrap;\n",
              "            }\n",
              "          </style>\n",
              "          "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "          <style>\n",
              "            pre {\n",
              "                white-space: pre-wrap;\n",
              "            }\n",
              "          </style>\n",
              "          "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.03335074,  0.06219646, -0.02878724, -0.06675987,  0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(arr2/arr1)-1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "9eJS1Qe0Egbe",
        "outputId": "fcf49bdc-154a-4f02-b673-6d3a1adc3a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "          <style>\n",
              "            pre {\n",
              "                white-space: pre-wrap;\n",
              "            }\n",
              "          </style>\n",
              "          "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "          <style>\n",
              "            pre {\n",
              "                white-space: pre-wrap;\n",
              "            }\n",
              "          </style>\n",
              "          "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-230-59e3e01a186d>:1: RuntimeWarning: invalid value encountered in true_divide\n",
            "  (arr2/arr1)-1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.93372198,  0.48415881, -0.25702422, -0.09223309,         nan])"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    }
  ]
}